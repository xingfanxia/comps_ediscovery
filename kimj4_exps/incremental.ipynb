{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is from \"big run\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from lib import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup\n",
    "lsa_np = np.load('../data/parsed/lsa_output.npy')\n",
    "\n",
    "metadata = pd.read_pickle('../data/parsed/pickles/pickled_data_test.pickle')\n",
    "metadata = metadata.loc[metadata['Scenario'] == '401']\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "lsa_df = pd.DataFrame(lsa_np)\n",
    "\n",
    "df = pd.concat([metadata, lsa_df], axis=1, join_axes=[metadata.index])\n",
    "df = df.loc[df['Label'] != '-1']\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "cat_features = ['To','From']\n",
    "features = list(range(100))\n",
    "features.extend(cat_features + ['Date'])\n",
    "\n",
    "df = df[features + ['Label'] + ['ID']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in incremental learning vs trees training on larger initial sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using params from sweep\n",
    "\n",
    "n_trees = 32\n",
    "tree_depth = 70\n",
    "random_seed = 42\n",
    "n_max_features = 90\n",
    "cat_features = ['To', 'From']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forests Trained on increasing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_100 = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "forest_200 = RNF(df[:200], n_trees, tree_depth, random_seed, n_max_features, 200, cat_features)\n",
    "forest_300 = RNF(df[:300], n_trees, tree_depth, random_seed, n_max_features, 300, cat_features)\n",
    "forest_400 = RNF(df[:400], n_trees, tree_depth, random_seed, n_max_features, 400, cat_features)\n",
    "forest_500 = RNF(df[:500], n_trees, tree_depth, random_seed, n_max_features, 500, cat_features)\n",
    "incremental_forests = [forest_100, forest_200, forest_300, forest_400, forest_500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forest in incremental_forests:\n",
    "    forest.fit_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "for forest in incremental_forests:\n",
    "    print(evalStats(forest.predict_parallel(df[-100:])[1], df[-100:]), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100, 200, ..., 500 document - trained forests\n",
    "Param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Incremental Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_forest = RNF(df[0:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_forest.fit_parallel()\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incremental_forest.update(df[100:200])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[200:300])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[300:400])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[400:500])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_forest.train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_500.train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing limited core usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = RNF(df[0:500], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "f.fit_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.trees[20].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 400 incremented vs 400 initial trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incremental forest initially trained on 100 rows\n",
    "inc_forest = RNF(df[0:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "inc_forest.fit_parallel();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_forest = RNF(df[:-100], n_trees, tree_depth, random_seed, n_max_features, 400, cat_features)\n",
    "init_forest.fit_parallel();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparing Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by Committee\n",
    "Asking for labeling on rows with the most amount of disagreement (between trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_rnf = RNF(df[:500], n_trees, tree_depth, random_seed, n_max_features, 500, cat_features)\n",
    "committee_rnf.fit_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = committee_rnf.predict_parallel(df[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreements = predictions[0]\n",
    "def entropy(pred):\n",
    "    s = 0\n",
    "    for x in pred[0]:\n",
    "        if x != 0:\n",
    "            s += x * math.log(x)\n",
    "    return (-1 * s, pred[1])\n",
    "\n",
    "# entropies = sorted(map(entropy, zip(agreements, predictions[2])), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def committee_increment():\n",
    "    # initial training\n",
    "    committee_rnf = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "    committee_rnf.fit_parallel()\n",
    "    \n",
    "    labeled_ids = df.loc[:100, 'ID'].values\n",
    "    test_ids = df.loc[-100:, 'ID'].values\n",
    "    labeled_ids = np.append(labeled_ids, test_ids)\n",
    "    test_results = committee_rnf.predict_parallel(df.loc[df['ID'].isin(test_ids)])\n",
    "    print(evalStats(test_results[1], df.loc[df['ID'].isin(test_ids)]))\n",
    "    for i in range(5):\n",
    "        # initial train scores\n",
    "\n",
    "        unlabeled_predict = committee_rnf.predict_parallel(df.loc[np.logical_not(df['ID'].isin(test_ids))])\n",
    "\n",
    "        to_label = sorted(map(entropy, zip(unlabeled_predict[0], unlabeled_predict[2])), reverse=True)[:100]\n",
    "\n",
    "        to_label_ids = [x[1] for x in to_label]\n",
    "\n",
    "        labeled_ids = np.append(labeled_ids, to_label_ids)\n",
    "\n",
    "        increment_df = df.loc[df[\"ID\"].isin(to_label_ids)]\n",
    "        \n",
    "        committee_rnf.update(increment_df)\n",
    "    \n",
    "        test_results = committee_rnf.predict_parallel(df.loc[df['ID'].isin(test_ids)])\n",
    "        print(evalStats(test_results[1], df.loc[df['ID'].isin(test_ids)]))\n",
    "        \n",
    "committee_increment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving towards training data homogeneity\n",
    "Trying to get Forst DF's relevant/irrelevant distribution to 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
