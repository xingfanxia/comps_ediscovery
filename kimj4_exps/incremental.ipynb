{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n"
     ]
    }
   ],
   "source": [
    "# The following is from \"big run\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from lib import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup\n",
    "lsa_np = np.load('../data/parsed/lsa_output.npy')\n",
    "\n",
    "metadata = pd.read_pickle('../data/parsed/pickles/pickled_data_test.pickle')\n",
    "metadata = metadata.loc[metadata['Scenario'] == '401']\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "lsa_df = pd.DataFrame(lsa_np)\n",
    "\n",
    "df = pd.concat([metadata, lsa_df], axis=1, join_axes=[metadata.index])\n",
    "df = df.loc[df['Label'] != '-1']\n",
    "df = df.reset_index(drop=True)\n",
    "print('first')\n",
    "cat_features = ['To','From']\n",
    "features = list(range(100))\n",
    "features.extend(cat_features + ['Date'])\n",
    "\n",
    "df = df[features + ['Label'] + ['ID']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in incremental learning vs trees training on larger initial sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using params from sweep\n",
    "\n",
    "# # temp params \n",
    "# n_trees = 32\n",
    "# tree_depth = 10\n",
    "# random_seed = 42\n",
    "# n_max_features = 20\n",
    "# cat_features = ['To', 'From']\n",
    "\n",
    "# optimal params\n",
    "# n_trees = 32\n",
    "# tree_depth = 90\n",
    "# random_seed = 42\n",
    "# n_max_features = 70\n",
    "# cat_features = ['To', 'From']\n",
    "\n",
    "n_trees = 32\n",
    "tree_depth = 70\n",
    "random_seed = 42\n",
    "n_max_features = 90\n",
    "cat_features = ['To', 'From']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forests Trained on increasing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_100 = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "forest_200 = RNF(df[:200], n_trees, tree_depth, random_seed, n_max_features, 200, cat_features)\n",
    "forest_300 = RNF(df[:300], n_trees, tree_depth, random_seed, n_max_features, 300, cat_features)\n",
    "forest_400 = RNF(df[:400], n_trees, tree_depth, random_seed, n_max_features, 400, cat_features)\n",
    "forest_500 = RNF(df[:500], n_trees, tree_depth, random_seed, n_max_features, 500, cat_features)\n",
    "forest_600 = RNF(df[:600], n_trees, tree_depth, random_seed, n_max_features, 600, cat_features)\n",
    "incremental_forests = [forest_100, forest_200, forest_300, forest_400, forest_500, forest_600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forest in incremental_forests:\n",
    "    forest.fit_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "for forest in incremental_forests:\n",
    "    print(evalStats(forest.predict_parallel(df[-100:])[1], df[-100:]), end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Regular Incremental Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_forest = RNF(df[0:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_forest.fit_parallel()\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_forest.update(df[100:200])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[200:300])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[300:400])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[400:500])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[500:600])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparing Query Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query by Committee\n",
    "Asking for labeling on rows with the most amount of disagreement (between trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "committee_rnf = RNF(df[:500], n_trees, tree_depth, random_seed, n_max_features, 500, cat_features)\n",
    "committee_rnf.fit_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = committee_rnf.predict_parallel(df[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agreements = predictions[0]\n",
    "def entropy(pred):\n",
    "    s = 0\n",
    "    for x in pred[0]:\n",
    "        if x != 0:\n",
    "            s += x * math.log(x)\n",
    "    return (-1 * s, pred[1])\n",
    "\n",
    "# entropies = sorted(map(entropy, zip(agreements, predictions[2])), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5142857142857142, 0.9473684210526315, 0.82, 0.6666666666666666)\n",
      "before restructuring: there is a 0-row node\n",
      "before restructuring: there is a 0-row node\n",
      "before restructuring: there is a 0-row node\n",
      "before restructuring: there is a 0-row node\n",
      "(0.5428571428571428, 0.76, 0.78, 0.6333333333333332)\n",
      "before restructuring: there is a 0-row node\n",
      "(0.7428571428571429, 0.65, 0.77, 0.6933333333333334)\n",
      "(0.8571428571428571, 0.6976744186046512, 0.82, 0.7692307692307693)\n",
      "before restructuring: there is a 0-row node\n",
      "before restructuring: there is a 0-row node\n",
      "(0.8, 0.6086956521739131, 0.75, 0.6913580246913581)\n",
      "before restructuring: there is a 0-row node\n",
      "before restructuring: there is a 0-row node\n",
      "before restructuring: there is a 0-row node\n",
      "(0.7714285714285715, 0.6585365853658537, 0.78, 0.7105263157894737)\n",
      "(0.8285714285714286, 0.6444444444444445, 0.78, 0.7250000000000001)\n",
      "(0.6571428571428571, 0.6571428571428571, 0.76, 0.6571428571428571)\n",
      "(0.6857142857142857, 0.6857142857142857, 0.78, 0.6857142857142857)\n",
      "(0.7428571428571429, 0.8387096774193549, 0.86, 0.787878787878788)\n",
      "(0.8285714285714286, 0.6904761904761905, 0.81, 0.7532467532467533)\n",
      "(0.8, 0.6511627906976745, 0.78, 0.7179487179487181)\n"
     ]
    }
   ],
   "source": [
    "def committee_increment():\n",
    "    # initial training\n",
    "    committee_rnf = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "    committee_rnf.fit_parallel()\n",
    "    \n",
    "    increment_size = 50\n",
    "    \n",
    "    test_set = df[-100:]\n",
    "    train_set = df[:-100]\n",
    "    \n",
    "    labeled_ids = df.loc[:100, 'ID'].values\n",
    "    #trying to use -100: with df.loc just returns everything, so we need to get a little tricky\n",
    "    test_ids = df.iloc[-100:, df.columns.get_loc('ID')].values\n",
    "    labeled_ids = np.append(labeled_ids, test_ids)\n",
    "    test_results = committee_rnf.predict_parallel(df.loc[df['ID'].isin(test_ids)])\n",
    "    print(evalStats(test_results[1], df.loc[df['ID'].isin(test_ids)]))\n",
    "    for i in range(((df.shape[0] - 100) // increment_size) - 1):\n",
    "        # initial train scores\n",
    "        unlabeled_predict = committee_rnf.predict_parallel(df.loc[np.logical_not(df['ID'].isin(labeled_ids))])\n",
    "\n",
    "        to_label = sorted(map(entropy, zip(unlabeled_predict[0], unlabeled_predict[2])), reverse=True)[:increment_size]\n",
    "\n",
    "        to_label_ids = [x[1] for x in to_label]\n",
    "\n",
    "        labeled_ids = np.append(labeled_ids, to_label_ids)\n",
    "\n",
    "        increment_df = df.loc[df[\"ID\"].isin(to_label_ids)]\n",
    "        \n",
    "        committee_rnf.update(increment_df)\n",
    "    \n",
    "        test_results = committee_rnf.predict_parallel(df.loc[df['ID'].isin(test_ids)])\n",
    "        print(evalStats(test_results[1], df.loc[df['ID'].isin(test_ids)]))\n",
    "        \n",
    "committee_increment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def committee_increment_copy():\n",
    "    # initial training\n",
    "    committee_rnf = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "    committee_rnf.fit_parallel()\n",
    "    \n",
    "    increment_size = 50\n",
    "    \n",
    "    test_set = df[-100:]\n",
    "    train_set = df[:-100]\n",
    "    \n",
    "    print('Initial performance')\n",
    "    print(evalStats(committee_rnf.predict_parallel(test_set)[1], test_set))\n",
    "    \n",
    "    for i in range(5):\n",
    "        to_predict_on = train_set[~train_set['ID'].isin(committee_rnf.train_data['ID'])].dropna()\n",
    "        print('the effective size of the train_set is {}'.format(to_predict_on.shape[0]))\n",
    "        predictions = committee_rnf.predict_parallel(to_predict_on)\n",
    "        \n",
    "        least_agreement = sorted(zip(predictions[0], predictions[2]), key= lambda x: entropy(x), reverse=True)[:100]\n",
    "        least_agreement_ids = [x[1] for x in least_agreement]\n",
    "#         print(most_confident_ids)\n",
    "        incrementing_set = train_set[train_set['ID'].isin(least_agreement_ids)]\n",
    "        committee_rnf.update(incrementing_set)\n",
    "        print(evalStats(committee_rnf.predict_parallel(test_set)[1], test_set))\n",
    "        \n",
    "committee_increment_copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Moving towards training data homogeneity\n",
    "Trying to get Forst DF's relevant/irrelevant distribution to 1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homogenous_increment():\n",
    "    '''Incremental training, but tries to give data points that even out the proportions of relevant vs not relevant\n",
    "        rows that the forest has'''\n",
    "    homogenous_rnf = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "    homogenous_rnf.fit_parallel()\n",
    "    \n",
    "    labeled_ids = df.loc[:100, 'ID'].values\n",
    "    #trying to use -100: with df.loc just returns everything, so we need to get a little tricky\n",
    "    test_ids = df.iloc[-100:, df.columns.get_loc('ID')].values\n",
    "    labeled_ids = np.append(labeled_ids, test_ids)\n",
    "    test_results = homogenous_rnf.predict_parallel(df.loc[df['ID'].isin(test_ids)])\n",
    "    print(evalStats(test_results[1], df.loc[df['ID'].isin(test_ids)]))\n",
    "    \n",
    "    # incremental steps\n",
    "    for i in range(5):\n",
    "        # add 100 rows at each increment\n",
    "        num_new_rows = 100\n",
    "            \n",
    "        # get ratio in the forest's training data\n",
    "        num_relevant = homogenous_rnf.train_data[homogenous_rnf.train_data['Label']=='1'].shape[0]\n",
    "        num_irrelevant = homogenous_rnf.train_data[homogenous_rnf.train_data['Label']=='0'].shape[0]\n",
    "        print('forest has {} relevant and {} irrelevant docs in the training set'.format(num_relevant, num_irrelevant))\n",
    "    \n",
    "        # look at the distribution of new data points\n",
    "        if (num_irrelevant > num_relevant):\n",
    "            print('here1')\n",
    "            # need to supplement relevant docs\n",
    "            difference = num_irrelevant - num_relevant\n",
    "            num_new_rel = difference + (num_new_rows - difference) // 2\n",
    "            num_new_irr = num_new_rows - num_new_rel\n",
    "        elif (num_relevant > num_irrelevant):\n",
    "            print('here2')\n",
    "            difference = num_relevant - num_irrelevant\n",
    "            num_new_irr = difference + (num_new_rows - difference) // 2\n",
    "            num_new_rel = num_new_rows - num_new_irr\n",
    "        else:\n",
    "            print('here3')\n",
    "            num_new_irr = num_new_rows // 2\n",
    "            num_new_rel = num_new_rows - num_new_irr\n",
    "        print('rel rows added: {}, irr rows added: {}'.format(num_new_rel, num_new_irr))\n",
    "        \n",
    "        # predict on all of the rest of the training set that hasn't been added to the forest\n",
    "#         unlabeled_predict = committee_rnf.predict_parallel(df.loc[np.logical_not(df['ID'].isin(labeled_ids))])\n",
    "        not_yet_labeled = df.loc[np.logical_not(df['ID'].isin(labeled_ids))]\n",
    "        not_yet_labeled_rel = not_yet_labeled[not_yet_labeled['Label'] == '1']\n",
    "        not_yet_labeled_irr = not_yet_labeled[not_yet_labeled['Label'] == '0']\n",
    "        \n",
    "        if not not_yet_labeled_rel.shape[0] >= num_new_rel:\n",
    "            continue\n",
    "        elif not not_yet_labeled_irr.shape[0] >= num_new_irr:\n",
    "            continue\n",
    "        else:\n",
    "#             to_add = .append(not_yet_labeled_rel, not_yet_labeled_irr)\n",
    "            rel_to_add = not_yet_labeled_rel[:num_new_rel]\n",
    "            irr_to_add = not_yet_labeled_irr[:num_new_irr]\n",
    "            to_add = rel_to_add.append(irr_to_add)\n",
    "            to_add_ids = to_add['ID'].values\n",
    "            \n",
    "            homogenous_rnf.update(to_add)\n",
    "            test_results = homogenous_rnf.predict_parallel(df.loc[df['ID'].isin(test_ids)])\n",
    "            print(evalStats(test_results[1], df.loc[df['ID'].isin(test_ids)]))\n",
    "            labeled_ids = np.append(labeled_ids, to_add_ids)\n",
    "            \n",
    "\n",
    "#         to_label = sorted(map(entropy, zip(unlabeled_predict[0], unlabeled_predict[2])), reverse=True)[:100]\n",
    "\n",
    "#         to_label_ids = [x[1] for x in to_label]\n",
    "\n",
    "#         labeled_ids = np.append(labeled_ids, to_label_ids)\n",
    "\n",
    "#         increment_df = df.loc[df[\"ID\"].isin(to_label_ids)]\n",
    "        \n",
    "#         committee_rnf.update(increment_df)\n",
    "    \n",
    "#         test_results = committee_rnf.predict_parallel(df.loc[df['ID'].isin(test_ids)])\n",
    "#         print(evalStats(test_results[1], df.loc[df['ID'].isin(test_ids)]))\n",
    "        \n",
    "        \n",
    "homogenous_increment()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results from 32/90/70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = [(0.37142857142857144, 0.9285714285714286, 0.77, 0.5306122448979592),\n",
    "       (0.6571428571428571, 0.7666666666666667, 0.81, 0.7076923076923077),\n",
    "       (0.7428571428571429, 0.7647058823529411, 0.83, 0.7536231884057971),\n",
    "       (0.6857142857142857, 0.7272727272727273, 0.8, 0.7058823529411764),\n",
    "       (0.8, 0.7, 0.81, 0.7466666666666666),\n",
    "       (0.8, 0.6829268292682927, 0.8, 0.736842105263158)]\n",
    "comm_quer = [(0.37142857142857144, 0.9285714285714286, 0.77, 0.5306122448979592),\n",
    "             (0.5428571428571428, 0.95, 0.83, 0.6909090909090908),\n",
    "             (0.7142857142857143, 0.6410256410256411, 0.76, 0.6756756756756757),\n",
    "             (0.6857142857142857, 0.631578947368421, 0.75, 0.6575342465753424),\n",
    "             (0.7142857142857143, 0.5813953488372093, 0.72, 0.6410256410256411),\n",
    "             (0.45714285714285713, 0.6153846153846154, 0.71, 0.5245901639344263)]\n",
    "hom_quer = [(0.37142857142857144, 0.9285714285714286, 0.77, 0.5306122448979592),\n",
    "            (0.6571428571428571, 0.6764705882352942, 0.77, 0.6666666666666666),\n",
    "            (0.7714285714285715, 0.6585365853658537, 0.78, 0.7105263157894737),\n",
    "            (0.6, 0.8076923076923077, 0.81, 0.6885245901639345),\n",
    "            (0.5714285714285714, 0.7142857142857143, 0.77, 0.634920634920635),\n",
    "            (0.5714285714285714, 0.7142857142857143, 0.77, 0.634920634920635)]\n",
    "mc_quer = [(0.37142857142857144, 0.9285714285714286, 0.77, 0.5306122448979592),\n",
    "           (0.4, 0.6666666666666666, 0.72, 0.5),\n",
    "           (0.5142857142857142, 0.72, 0.76, 0.6),\n",
    "           (0.6285714285714286, 0.6111111111111112, 0.73, 0.619718309859155),\n",
    "           (0.7428571428571429, 0.7222222222222222, 0.81, 0.732394366197183),\n",
    "           (0.8285714285714286, 0.6590909090909091, 0.79, 0.7341772151898734)]\n",
    "non_inc_quer = [(0.37142857142857144, 0.9285714285714286, 0.77, 0.5306122448979592),\n",
    "                (0.7428571428571429, 0.8125, 0.85, 0.7761194029850748),\n",
    "                (0.7428571428571429, 0.8125, 0.85, 0.7761194029850748),\n",
    "                (0.6285714285714286, 0.7333333333333333, 0.79, 0.6769230769230768),\n",
    "                (0.7142857142857143, 0.78125, 0.83, 0.7462686567164178),\n",
    "               (0.7142857142857143, 0.78125, 0.83, 0.7462686567164178)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_prec = [x[0] for x in inc]\n",
    "inc_rec = [x[1] for x in inc]\n",
    "inc_acc = [x[2] for x in inc]\n",
    "inc_f1 = [x[3] for x in inc]\n",
    "comm_prec = [x[0] for x in comm_quer]\n",
    "comm_rec = [x[1] for x in comm_quer]\n",
    "comm_acc = [x[2] for x in comm_quer]\n",
    "comm_f1 = [x[3] for x in comm_quer]\n",
    "hom_prec = [x[0] for x in hom_quer]\n",
    "hom_rec = [x[1] for x in hom_quer]\n",
    "hom_acc = [x[2] for x in hom_quer]\n",
    "hom_f1 = [x[3] for x in hom_quer]\n",
    "mc_prec = [x[0] for x in mc_quer]\n",
    "mc_rec = [x[1] for x in mc_quer]\n",
    "mc_acc = [x[2] for x in mc_quer]\n",
    "mc_f1 = [x[3] for x in mc_quer]\n",
    "non_inc_prec = [x[0] for x in non_inc_quer]\n",
    "non_inc_rec = [x[1] for x in non_inc_quer]\n",
    "non_inc_acc = [x[2] for x in non_inc_quer]\n",
    "non_inc_f1 = [x[3] for x in non_inc_quer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some data\n",
    "x = range(0,600,100)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "\n",
    "ax.plot(x, inc_acc, marker=\"^\", label=\"Incremental\")\n",
    "ax.plot(x, comm_acc, marker=\"^\", label=\"committee\")\n",
    "ax.plot(x, hom_acc, marker = \"^\", label=\"Homogenous\")\n",
    "ax.plot(x, mc_acc, marker = \"^\", label=\"Most Conf\")\n",
    "ax.plot(x, non_inc_acc, marker = \"^\", label=\"not incremental\")\n",
    "\n",
    "plt.legend(loc=3)\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some data\n",
    "x = range(0,600,100)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "\n",
    "ax.plot(x, inc_prec, marker=\"^\", label=\"Incremental\")\n",
    "ax.plot(x, comm_prec, marker=\"^\", label=\"committee\")\n",
    "ax.plot(x, hom_prec, marker = \"^\", label=\"Homogenous\")\n",
    "ax.plot(x, mc_prec, marker = \"^\", label=\"Most Conf\")\n",
    "ax.plot(x, non_inc_prec, marker = \"^\", label=\"not incremental\")\n",
    "\n",
    "plt.legend(loc=3)\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some data\n",
    "x = range(0,600,100)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "\n",
    "ax.plot(x, inc_rec, marker=\"^\", label=\"Incremental\")\n",
    "ax.plot(x, comm_rec, marker=\"^\", label=\"committee\")\n",
    "ax.plot(x, hom_rec, marker = \"^\", label=\"Homogenous\")\n",
    "ax.plot(x, mc_rec, marker = \"^\", label=\"Most Conf\")\n",
    "ax.plot(x, non_inc_rec, marker = \"^\", label=\"not incremental\")\n",
    "\n",
    "plt.legend(loc=3)\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot some data\n",
    "x = range(0,600,100)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "\n",
    "ax.plot(x, inc_f1, marker=\"^\", label=\"Incremental\")\n",
    "ax.plot(x, comm_f1, marker=\"^\", label=\"committee\")\n",
    "ax.plot(x, hom_f1, marker = \"^\", label=\"Homogenous\")\n",
    "ax.plot(x, mc_f1, marker = \"^\", label=\"Most Conf\")\n",
    "ax.plot(x, non_inc_f1,marker = \"^\", label=\"not incremental\")\n",
    "\n",
    "plt.legend(loc=3)\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incrementing with the most confident data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial performance\n",
      "(0.5142857142857142, 0.9473684210526315, 0.82, 0.6666666666666666)\n",
      "the effective size of the train_set is 510\n",
      "node has left child but no right\n",
      "node has left child but no right\n"
     ]
    }
   ],
   "source": [
    "def most_confident_increment():\n",
    "    '''\n",
    "    Increments by 100 each run. Takes the 100 most confident data points to use add to the training set\n",
    "    '''\n",
    "    mc_rnf = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "    mc_rnf.fit_parallel()\n",
    "    \n",
    "    test_set = df[-100:]\n",
    "    train_set = df[:-100]\n",
    "    \n",
    "    print('Initial performance')\n",
    "    print(evalStats(mc_rnf.predict_parallel(test_set)[1], test_set))\n",
    "    \n",
    "    for i in range(5):\n",
    "        to_predict_on = train_set[~train_set['ID'].isin(mc_rnf.train_data['ID'])].dropna()\n",
    "        print('the effective size of the train_set is {}'.format(to_predict_on.shape[0]))\n",
    "        predictions = mc_rnf.predict_parallel(to_predict_on)\n",
    "        \n",
    "        most_confident = sorted(zip(predictions[0], predictions[2]), key= lambda x: abs(x[0][1] - x[0][0]), reverse=True)[:100]\n",
    "        \n",
    "        most_confident_ids = [x[1] for x in most_confident]\n",
    "        \n",
    "#         print(most_confident_ids)\n",
    "        incrementing_set = train_set[train_set['ID'].isin(most_confident_ids)]\n",
    "        mc_rnf.update(incrementing_set)\n",
    "        print(evalStats(mc_rnf.predict_parallel(test_set)[1], test_set))\n",
    "    \n",
    "most_confident_increment()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node.get_proportions divide-by-zero debugging log  \n",
    "- Are all nodes in a tree getting reset? (proportions, rows being reset) - - - yes\n",
    "- Does the issue recreate if forest.predict_parallel is called with the serial perdict code? - - - yes\n",
    "- If the tree needs to be restructured, does it end up with fewer nodes than it started with? - - - yes\n",
    "- When some nodes are resturctured, is the correct new node being added to the list to be examined? - - - maybe\n",
    "- To which nodes does it happen? - - - just leaves\n",
    "- What if all of a data goes to one child? - - - THIS DOENS'T WORK!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
