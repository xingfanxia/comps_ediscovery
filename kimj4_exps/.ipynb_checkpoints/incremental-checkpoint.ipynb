{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is from \"big run\"\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from lib import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lib.dblib import Database\n",
    "\n",
    "db = dblib.Database()\n",
    "\n",
    "# Setup\n",
    "lsa_np = np.load('../data/parsed/lsa_output.npy')\n",
    "metadata = db.df_from_table('emails')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata = metadata.loc[metadata['Scenario'] == '401']\n",
    "metadata = metadata.reset_index(drop=True)\n",
    "lsa_df = pd.DataFrame(lsa_np)\n",
    "df = pd.concat([metadata, lsa_df], axis=1, join_axes=[metadata.index])\n",
    "df = df.loc[df['Relevant'] != '-1']\n",
    "df = df.reset_index(drop=True)\n",
    "cat_features = ['To','From']\n",
    "features = list(range(100))\n",
    "features.extend(cat_features + ['Date'])\n",
    "# features.extend(cat_features + ['ID'])\n",
    "\n",
    "df = df[features + ['Label'] + ['Relevant'] + ['ID']]\n",
    "# df = df[features + ['ID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in incremental learning vs trees training on larger initial sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using params from sweep\n",
    "\n",
    "n_trees = 32\n",
    "tree_depth = 70\n",
    "random_seed = 42\n",
    "n_max_features = 90\n",
    "cat_features = ['To', 'From']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forests Trained on increasing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_100 = RNF(df[:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "forest_200 = RNF(df[:200], n_trees, tree_depth, random_seed, n_max_features, 200, cat_features)\n",
    "forest_300 = RNF(df[:300], n_trees, tree_depth, random_seed, n_max_features, 300, cat_features)\n",
    "forest_400 = RNF(df[:400], n_trees, tree_depth, random_seed, n_max_features, 400, cat_features)\n",
    "forest_500 = RNF(df[:500], n_trees, tree_depth, random_seed, n_max_features, 500, cat_features)\n",
    "incremental_forests = [forest_100, forest_200, forest_300, forest_400, forest_500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when splitting categorically, no best address found: [56 34 39]\n",
      "when splitting categorically, no best address found: [323 402 318]\n"
     ]
    }
   ],
   "source": [
    "for forest in incremental_forests:\n",
    "    forest.fit_parallel()"
=======
   "outputs": [],
   "source": [
    "n_trees = 64\n",
    "tree_depth = 10\n",
    "random_seed = 42\n",
    "n_max_features = 11\n",
    "cat_features = ['To', 'From']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forests Trained on increasing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_100 = RNF(df[:-100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "forest_200 = RNF(df[:-100], n_trees, tree_depth, random_seed, n_max_features, 200, cat_features)\n",
    "forest_300 = RNF(df[:-100], n_trees, tree_depth, random_seed, n_max_features, 300, cat_features)\n",
    "forest_400 = RNF(df[:-100], n_trees, tree_depth, random_seed, n_max_features, 400, cat_features)\n",
    "forest_500 = RNF(df[:-100], n_trees, tree_depth, random_seed, n_max_features, 500, cat_features)\n",
    "incremental_forests = [forest_100, forest_200, forest_300, forest_400, forest_500]"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.45714285714285713, 0.9411764705882353, 0.8, 0.6153846153846154)\n",
      "\n",
      "(0.7142857142857143, 0.8064516129032258, 0.84, 0.7575757575757576)\n",
      "\n",
      "(0.7428571428571429, 0.7428571428571429, 0.82, 0.7428571428571429)\n",
      "\n",
      "(0.7428571428571429, 0.7647058823529411, 0.83, 0.7536231884057971)\n",
      "\n",
      "(0.6857142857142857, 0.75, 0.81, 0.7164179104477612)\n",
      "\n"
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "# evaluation\n",
    "for forest in incremental_forests:\n",
    "    print(evalStats(forest.predict_parallel(df[-100:])[1], df[-100:]), end='\\n\\n')"
=======
    "for forest in incremental_forests:\n",
    "    forest.fit_parallel()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "100, 200, ..., 500 document - trained forests\n",
    "Param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Incremental Forests"
=======
    "forest_500.fit_parallel()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "incremental_forest = RNF(df[0:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)"
=======
    "print(evalStats(forest_500.predict_parallel(df[-100:])[1], df[-100:]), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "for forest in incremental_forests:\n",
    "    print(evalStats(forest.predict_parallel(df[-100:])[1], df[-100:]), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with 64, 100, 42, 11:  \n",
    "(0.8285714285714286, 0.8055555555555556, 0.87, 0.8169014084507044)  \n",
    "(0.6285714285714286, 0.9565217391304348, 0.86, 0.7586206896551724)  \n",
    "(0.2571428571428571, 0.75, 0.71, 0.3829787234042553)  \n",
    "(0.2, 0.7777777777777778, 0.7, 0.3181818181818182)  \n",
    "(0.02857142857142857, 0.3333333333333333, 0.64, 0.05263157894736842)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with 64, 10, 42, 11:  \n",
    "(0.7428571428571429, 0.7878787878787878, 0.84, 0.7647058823529412)  \n",
    "(0.8, 0.7368421052631579, 0.83, 0.7671232876712328)  \n",
    "(0.9142857142857143, 0.5333333333333333, 0.69, 0.6736842105263158)  \n",
    "(0.9714285714285714, 0.4788732394366197, 0.62, 0.6415094339622641)  \n",
    "(0.9714285714285714, 0.4857142857142857, 0.63, 0.6476190476190476)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Incremental Forests"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when splitting categorically, no best address found: [56 34 39]\n",
      "(0.45714285714285713, 0.9411764705882353, 0.8, 0.6153846153846154)\n"
     ]
    }
   ],
=======
   "outputs": [],
   "source": [
    "incremental_forest = RNF(df[0:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "incremental_forest.fit_parallel()\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "(0.7714285714285715, 0.9, 0.89, 0.8307692307692307)\n",
      "23\n",
      "(0.8571428571428571, 0.8571428571428571, 0.9, 0.8571428571428571)\n",
      "28\n",
      "(0.7714285714285715, 0.84375, 0.87, 0.8059701492537314)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "incremental_forest.update(df[100:200])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[200:300])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[300:400])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[400:500])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incremental_forest.train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_500.train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing limited core usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = RNF(df[0:500], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "f.fit_parallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.trees[20].visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 400 incremented vs 400 initial trained"
=======
   "outputs": [],
   "source": [
    "incremental_forest.update(df[100:200])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[200:300])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[300:400])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))\n",
    "\n",
    "incremental_forest.update(df[400:500])\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_forest.update(df[500:600])\n",
    "print(len(set(incremental_forest.trees)))\n",
    "print(evalStats(incremental_forest.predict_parallel(df[-100:])[1], df[-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing limited core usage"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incremental forest initially trained on 100 rows\n",
    "inc_forest = RNF(df[0:100], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "inc_forest.fit_parallel();"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = RNF(df[0:500], n_trees, tree_depth, random_seed, n_max_features, 100, cat_features)\n",
    "f.fit_parallel()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_forest = RNF(df[:-100], n_trees, tree_depth, random_seed, n_max_features, 400, cat_features)\n",
    "init_forest.fit_parallel();"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.trees[20].visualize()"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
