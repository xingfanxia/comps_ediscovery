{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"#What-is-Random-Forest\" data-toc-modified-id=\"What-is-Random-Forest-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>What is Random Forest</a></span></li><li><span><a href=\"#The-algorithm\" data-toc-modified-id=\"The-algorithm-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The algorithm</a></span></li><li><span><a href=\"#Data-Prep\" data-toc-modified-id=\"Data-Prep-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Prep</a></span></li><li><span><a href=\"#Decision-Tree-and-Random-Forest\" data-toc-modified-id=\"Decision-Tree-and-Random-Forest-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Decision Tree and Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-tree-basics\" data-toc-modified-id=\"Decision-tree-basics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Decision tree basics</a></span></li><li><span><a href=\"#Implementation-of-Tree\" data-toc-modified-id=\"Implementation-of-Tree-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Implementation of Tree</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Decision trees can suffer from high variance which makes their results fragile to the specific training data used.\n",
    "\n",
    ">Building multiple models from samples of your training data, called bagging, can reduce this variance, but the trees are highly correlated.\n",
    "\n",
    ">Random Forest is an extension of bagging that in addition to building trees based on multiple samples of your training data, it also constrains the features that can be used to build the trees, forcing trees to be different. This, in turn, can give a lift in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Decision trees involve the greedy selection of the best split point from the dataset at each step.\n",
    "\n",
    ">This algorithm makes decision trees susceptible to high variance if they are not pruned. This high variance can be harnessed and reduced by creating multiple trees with different samples of the training dataset (different views of the problem) and combining their predictions. This approach is called bootstrap aggregation or bagging for short.\n",
    "\n",
    ">A limitation of bagging is that the same greedy algorithm is used to create each tree, meaning that it is likely that the same or very similar split points will be chosen in each tree making the different trees very similar (trees will be correlated). This, in turn, makes their predictions similar, mitigating the variance originally sought.\n",
    "\n",
    ">We can force the decision trees to be different by limiting the features (rows) that the greedy algorithm can evaluate at each split point when creating the tree. This is called the Random Forest algorithm.\n",
    "\n",
    ">Like bagging, multiple samples of the training dataset are taken and a different tree trained on each. The difference is that at each point a split is made in the data and added to the tree, only a fixed subset of attributes can be considered.\n",
    "\n",
    ">For classification problems,  the number of attributes to be considered for the split is limited to the square root of the number of input features.\n",
    "\n",
    ">The result of this one small change are trees that are more different from each other (uncorrelated) resulting predictions that are more diverse and a combined prediction that often has better performance that single tree or bagging alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data used is the sonar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:02:47.502773Z",
     "start_time": "2017-10-17T17:02:47.386017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data/research: File exists\r\n",
      "mkdir: -p: File exists\r\n"
     ]
    }
   ],
   "source": [
    "%mkdir data/research -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:04:34.712910Z",
     "start_time": "2017-10-17T17:04:34.673041Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'request' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-63df49dab62f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/research/sonar.all-data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'request' is not defined"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "file_path = 'data/research/sonar.all-data.csv'\n",
    "d_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data'\n",
    "request.urlretrieve(d_url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:25.220917Z",
     "start_time": "2017-10-17T17:05:24.662241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(file_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:25.824825Z",
     "start_time": "2017-10-17T17:05:25.814453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decision tree, split points are chosed by finding the feature and the value of that feature which results in lowerst cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problem, this cost is usually evaluated by a cost function called Gini index. Gini index calculates the purity of the group of data created by the split point.\n",
    "\n",
    "A tree node is pure (`gini = 0`) if all instances it applies to belong to the same class.\n",
    "\n",
    "*Gini Impurity* is measured as \n",
    "$$\n",
    "G_i = 1 - \\sum_{k=1}^n p_{i,k}^2\n",
    "$$\n",
    "\n",
    "where $p_{i,k}$ refers to the ratio of class $k$  instances among the whole input instances in the $i^{th}$ node.\n",
    "\n",
    "For example, assume there is a node with 54 input instances, 0 of them belong to class A, 49 of them belong to class B, and 5 of them belong to class C. Then the gini score is $1 - (0/54)^2 - (49/54)^2 - (5/54)^2 \\approx 0.168$\n",
    "\n",
    "In our case, we only want a binary classifier outputing `relevant (1)` or `irrelavent (0)`. So if a node perfectly separated the input into one class(leaf), the *gini impurity* will be 0.\n",
    "\n",
    "Another measure will be *Entropy*:\n",
    "$$H_i = - \\sum_{k=1 \\mid p_{i,k} \\neq 0}^n p_{i,k}log(p_{i,k})$$\n",
    "Note *Entropy* is more expensive as it uses $log$.\n",
    "\n",
    "> - Gini is intended for continuous attributes, and Entropy for attributes that occur in classes\n",
    "- Gini is to minimize misclassification\n",
    "- Entropy is for exploratory analysis\n",
    "- Entropy may be a little slower to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:24:46.529728Z",
     "start_time": "2017-10-12T16:24:46.515712Z"
    }
   },
   "source": [
    "General Implementation of both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:38.380220Z",
     "start_time": "2017-10-17T17:05:38.365889Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_shannon_entropy(self, left, right):\n",
    "        left_sum = sum(left.values())\n",
    "        right_sum = sum(right.values())\n",
    "        if 0 in left.values():\n",
    "            left_entropy = 0\n",
    "        else:\n",
    "            left_entropy = sum([-(i/left_sum)*np.log2(i/left_sum) for i in left.values()])\n",
    "\n",
    "        if 0 in right.values():\n",
    "            right_entropy = 0\n",
    "        else:\n",
    "            right_entropy = sum([-(i/right_sum)*np.log2(i/right_sum) for i in right.values()])\n",
    "        entropy = (left_entropy*left_sum + right_entropy*right_sum) / (left_sum + right_sum)\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:38.658171Z",
     "start_time": "2017-10-17T17:05:38.654286Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_gini_index(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:38.953897Z",
     "start_time": "2017-10-17T17:05:38.951196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:06:45.776850Z",
     "start_time": "2017-10-17T17:06:45.570177Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A dummy version of tree nodes\n",
    "'''\n",
    "class Node:\n",
    "      \n",
    "    def __init__(self, data, rows, features):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = data\n",
    "        self.rows = rows\n",
    "        self.features = features\n",
    "        self.label_index = 60\n",
    "        self.labels = ['R', 'M']\n",
    "        self.spliting_feature_val = None\n",
    "\n",
    "    \n",
    "    def calc_shannon_entropy(self):\n",
    "        raw_val = 0\n",
    "        for label in self.labels:\n",
    "            members = self.data.loc[self.data[self.label_index] == label]\n",
    "            if len(members) <= 0: continue\n",
    "            filtered = [x for x in members.index.values if x in self.rows]\n",
    "            intermediate = len(filtered)/len(self.rows)\n",
    "            raw_val += -intermediate*np.log2(intermediate)\n",
    "        return raw_val\n",
    "    \n",
    "    def calc_gini_index(self):\n",
    "        raw_val = 1\n",
    "        for label in self.labels:\n",
    "            members = self.data.loc[self.data[self.label_index] == label]\n",
    "            filtered = [x for x in members.index.values if x in self.rows]\n",
    "#             filtered = members\n",
    "            raw_val -= (len(filtered)/len(self.rows))**2\n",
    "        return raw_val\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    calculate info gain from gini/entropy\n",
    "    '''\n",
    "    def cal_info_gain():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def find_break_points(self, df, feature):\n",
    "        breaks = []\n",
    "        for i in range(len(df)-1):\n",
    "            row = df[i:i+1]\n",
    "            next_row = df[i+1:i+2]\n",
    "#             print(row[self.label_index])\n",
    "            if row[self.label_index].values[0] != next_row[self.label_index].values[0]:\n",
    "                breaks.append(next_row[feature].values[0]) #float precision issue, care\n",
    "        return breaks\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    Choose the best feature to split at this point\n",
    "    i.e. low gini/entropy, high infoGain\n",
    "    '''\n",
    "    \n",
    "    def split(self):\n",
    "        min_gini, min_feature, min_break_point, new_left, new_right = 1, -999, -999, None, None\n",
    "        for feature in self.features:\n",
    "#             print('parsing')\n",
    "            to_parse = self.data[[feature, self.label_index]]\n",
    "            to_parse = to_parse.loc[to_parse.index.isin (self.rows)]\n",
    "            to_parse.sort_values(feature, inplace=True)\n",
    "#             print(to_parse)\n",
    "            break_points = self.find_break_points(to_parse, feature)\n",
    "            for break_point in break_points:\n",
    "                left = Node(self.data, to_parse.loc[to_parse[feature] < break_point].index.values, [x for x in self.features if x != feature])\n",
    "#                 left = Node(to_parse.loc[to_parse[feature] <break_point]\n",
    "                right = Node(self.data, to_parse.loc[to_parse[feature] >= break_point].index.values, [x for x in self.features if x != feature])\n",
    "                ## We should ajdust this so it pass self.data and reference of rows and cols\n",
    "#                 print(left.index.values)\n",
    "\n",
    "#                 print(self.calc_gini_index(left))\n",
    "                total_gini = (left.calc_gini_index()*len(left.rows) + right.calc_gini_index()*len(right.rows) )/(len(left.rows) + len(right.rows) )\n",
    "#                 min_gini = min(total_gini, min_gini)\n",
    "                if total_gini < min_gini:\n",
    "                    min_gini, min_break_point, min_feature, new_left, new_right = total_gini, break_point, feature, left, right\n",
    "        self.left = new_left\n",
    "        self.right = new_right\n",
    "        self.spliting_feature_val = (min_feature, min_break_point, min_gini)\n",
    "        return(min_gini, min_break_point, min_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:07:21.996457Z",
     "start_time": "2017-10-17T17:06:46.020532Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.36504117616970283, 0.19889999999999999, 10)\n",
      "(0.2577001542518784, 0.052499999999999998, 3)\n"
     ]
    }
   ],
   "source": [
    "ls = [x for x in range(60)]\n",
    "dummy = Node(df, range(df.shape[0]), ls)\n",
    "# dummy = Node(df, [3,5,6,7], range(60))\n",
    "print(dummy.split())\n",
    "print(dummy.left.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 64, 67, 65, 50, 10])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.left.rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.49120158267388586, 0.088599999999999998, 16)\n"
     ]
    }
   ],
   "source": [
    "print(dummy.right.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:08:09.244407Z",
     "start_time": "2017-10-17T17:07:52.015850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.29522920871894476, 0.66990000000000005, 15)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b69de2c05a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-652da7cd1e08>\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m#                 print(self.calc_gini_index(left))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mtotal_gini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_gini_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_gini_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;31m#                 min_gini = min(total_gini, min_gini)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtotal_gini\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_gini\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-652da7cd1e08>\u001b[0m in \u001b[0;36mcalc_gini_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#             filtered = members\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mraw_val\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mraw_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "head = dummy\n",
    "while (i < 10):\n",
    "    dummy = dummy.right\n",
    "    print(dummy.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T03:09:58.069975Z",
     "start_time": "2017-10-15T03:09:58.052936Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A dummy implementation of decision trees\n",
    "'''\n",
    "class Tree:\n",
    "    \n",
    "    '''\n",
    "    params:\n",
    "    train_data - training data to trainthe tree\n",
    "    depth - max recursion depth of the tree\n",
    "    benchmark - benchmark for geni/entropy\n",
    "    '''\n",
    "    def __init__(train_data, depth, benchmark): #should we include data here\n",
    "        self.depth = depth\n",
    "        \n",
    "    '''\n",
    "    Recursively split until geni/entropy benchmark met or max_depth reached\n",
    "    '''\n",
    "    def fit(train_data):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    params: \n",
    "    test_data - test data to run the prediction on\n",
    "    \n",
    "    return: \n",
    "    outputs confidence/probability of each category\n",
    "    '''\n",
    "    def predict(test_data):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    params: \n",
    "    more_data - more training data to update the tree\n",
    "    \n",
    "    return: \n",
    "    Null or we can say something like which nodes are changed\n",
    "    '''\n",
    "    def update(more_data):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    Maybe we can use pickle for this\n",
    "    '''\n",
    "    def store_tree(file_path):\n",
    "        pass\n",
    "    \n",
    "    def load_tree(file_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T03:09:58.617626Z",
     "start_time": "2017-10-15T03:09:58.578957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dummy Version of Random Forest\n",
    "'''\n",
    "class RNF: \n",
    "    '''\n",
    "    params:\n",
    "    train_data - training data to trainthe tree\n",
    "    n_trees - number of trees to setup\n",
    "    tree_depth - max recursive\n",
    "    random_seed - seed for random gen\n",
    "    n_max_features - max num of features to pass to each tree\n",
    "    n_max_input - max num of input to pass to each tree\n",
    "    '''\n",
    "    def __init__(train_data, n_trees, tree_depth, random_seed, n_max_features, n_max_input):\n",
    "        init(trees) \n",
    "        self.trees = trees\n",
    "        self.features = [()] #list of tuples like (tree, emails, features)\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    Randomly select features and emails from the train_data \n",
    "    '''\n",
    "    def random_select(train_data):\n",
    "        pass\n",
    "        \n",
    "    '''\n",
    "    pass randomly selected emails and features to each tree\n",
    "    '''\n",
    "    def fit():\n",
    "        for tree in trees:\n",
    "            tree.fit(random_select(train_data))\n",
    "    \n",
    "    '''\n",
    "    calculate a proba from output of each tree's prediction\n",
    "    should ouput two arrays: probas and classfication\n",
    "    '''\n",
    "    def some_majority_count_metric():\n",
    "        pass\n",
    "    \n",
    "    def predict(test_data):\n",
    "        scores = [tree.predict(test_data) for tree in trees]\n",
    "        return some_majority_count_metric(scores)\n",
    "    \n",
    "    '''\n",
    "    params: \n",
    "    more_data - more training data to update the forest\n",
    "    \n",
    "    return: \n",
    "    Null or we can say something like which trees are changed\n",
    "    '''\n",
    "    def update(more_data):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    Maybe we can use pickle for this\n",
    "    '''\n",
    "    def store_rnf(file_path):\n",
    "        pass\n",
    "    \n",
    "    def load_rnf(file_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
