{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"#What-is-Random-Forest\" data-toc-modified-id=\"What-is-Random-Forest-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>What is Random Forest</a></span></li><li><span><a href=\"#The-algorithm\" data-toc-modified-id=\"The-algorithm-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The algorithm</a></span></li><li><span><a href=\"#Data-Prep\" data-toc-modified-id=\"Data-Prep-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Prep</a></span></li><li><span><a href=\"#Decision-Tree-and-Random-Forest\" data-toc-modified-id=\"Decision-Tree-and-Random-Forest-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Decision Tree and Random Forest</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-tree-basics\" data-toc-modified-id=\"Decision-tree-basics-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Decision tree basics</a></span></li><li><span><a href=\"#Implementation-of-Tree\" data-toc-modified-id=\"Implementation-of-Tree-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Implementation of Tree</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Decision trees can suffer from high variance which makes their results fragile to the specific training data used.\n",
    "\n",
    ">Building multiple models from samples of your training data, called bagging, can reduce this variance, but the trees are highly correlated.\n",
    "\n",
    ">Random Forest is an extension of bagging that in addition to building trees based on multiple samples of your training data, it also constrains the features that can be used to build the trees, forcing trees to be different. This, in turn, can give a lift in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Decision trees involve the greedy selection of the best split point from the dataset at each step.\n",
    "\n",
    ">This algorithm makes decision trees susceptible to high variance if they are not pruned. This high variance can be harnessed and reduced by creating multiple trees with different samples of the training dataset (different views of the problem) and combining their predictions. This approach is called bootstrap aggregation or bagging for short.\n",
    "\n",
    ">A limitation of bagging is that the same greedy algorithm is used to create each tree, meaning that it is likely that the same or very similar split points will be chosen in each tree making the different trees very similar (trees will be correlated). This, in turn, makes their predictions similar, mitigating the variance originally sought.\n",
    "\n",
    ">We can force the decision trees to be different by limiting the features (rows) that the greedy algorithm can evaluate at each split point when creating the tree. This is called the Random Forest algorithm.\n",
    "\n",
    ">Like bagging, multiple samples of the training dataset are taken and a different tree trained on each. The difference is that at each point a split is made in the data and added to the tree, only a fixed subset of attributes can be considered.\n",
    "\n",
    ">For classification problems,  the number of attributes to be considered for the split is limited to the square root of the number of input features.\n",
    "\n",
    ">The result of this one small change are trees that are more different from each other (uncorrelated) resulting predictions that are more diverse and a combined prediction that often has better performance that single tree or bagging alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample data used is the sonar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:02:47.502773Z",
     "start_time": "2017-10-17T17:02:47.386017Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir -p data/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:04:34.712910Z",
     "start_time": "2017-10-17T17:04:34.673041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/research/sonar.all-data.csv', <http.client.HTTPMessage at 0x1031df9b0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request as request\n",
    "file_path = 'data/research/sonar.all-data.csv'\n",
    "d_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data'\n",
    "request.urlretrieve(d_url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:25.220917Z",
     "start_time": "2017-10-17T17:05:24.662241Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(file_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:25.824825Z",
     "start_time": "2017-10-17T17:05:25.814453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a decision tree, split points are chosed by finding the feature and the value of that feature which results in lowerst cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problem, this cost is usually evaluated by a cost function called Gini index. Gini index calculates the purity of the group of data created by the split point.\n",
    "\n",
    "A tree node is pure (`gini = 0`) if all instances it applies to belong to the same class.\n",
    "\n",
    "*Gini Impurity* is measured as \n",
    "$$\n",
    "G_i = 1 - \\sum_{k=1}^n p_{i,k}^2\n",
    "$$\n",
    "\n",
    "where $p_{i,k}$ refers to the ratio of class $k$  instances among the whole input instances in the $i^{th}$ node.\n",
    "\n",
    "For example, assume there is a node with 54 input instances, 0 of them belong to class A, 49 of them belong to class B, and 5 of them belong to class C. Then the gini score is $1 - (0/54)^2 - (49/54)^2 - (5/54)^2 \\approx 0.168$\n",
    "\n",
    "In our case, we only want a binary classifier outputing `relevant (1)` or `irrelavent (0)`. So if a node perfectly separated the input into one class(leaf), the *gini impurity* will be 0.\n",
    "\n",
    "Another measure will be *Entropy*:\n",
    "$$H_i = - \\sum_{k=1 \\mid p_{i,k} \\neq 0}^n p_{i,k}log(p_{i,k})$$\n",
    "Note *Entropy* is more expensive as it uses $log$.\n",
    "\n",
    "> - Gini is intended for continuous attributes, and Entropy for attributes that occur in classes\n",
    "- Gini is to minimize misclassification\n",
    "- Entropy is for exploratory analysis\n",
    "- Entropy may be a little slower to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-12T16:24:46.529728Z",
     "start_time": "2017-10-12T16:24:46.515712Z"
    }
   },
   "source": [
    "General Implementation of both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:38.380220Z",
     "start_time": "2017-10-17T17:05:38.365889Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_shannon_entropy(self, left, right):\n",
    "        left_sum = sum(left.values())\n",
    "        right_sum = sum(right.values())\n",
    "        if 0 in left.values():\n",
    "            left_entropy = 0\n",
    "        else:\n",
    "            left_entropy = sum([-(i/left_sum)*np.log2(i/left_sum) for i in left.values()])\n",
    "\n",
    "        if 0 in right.values():\n",
    "            right_entropy = 0\n",
    "        else:\n",
    "            right_entropy = sum([-(i/right_sum)*np.log2(i/right_sum) for i in right.values()])\n",
    "        entropy = (left_entropy*left_sum + right_entropy*right_sum) / (left_sum + right_sum)\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:38.658171Z",
     "start_time": "2017-10-17T17:05:38.654286Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_gini_index(data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:05:38.953897Z",
     "start_time": "2017-10-17T17:05:38.951196Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0368 R\n",
      "         0       1       2       3       4       5       6       7       8   \\\n",
      "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
      "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
      "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
      "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
      "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
      "5    0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
      "6    0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
      "7    0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
      "8    0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
      "9    0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
      "10   0.0039  0.0063  0.0152  0.0336  0.0310  0.0284  0.0396  0.0272  0.0323   \n",
      "11   0.0123  0.0309  0.0169  0.0313  0.0358  0.0102  0.0182  0.0579  0.1122   \n",
      "12   0.0079  0.0086  0.0055  0.0250  0.0344  0.0546  0.0528  0.0958  0.1009   \n",
      "13   0.0090  0.0062  0.0253  0.0489  0.1197  0.1589  0.1392  0.0987  0.0955   \n",
      "14   0.0124  0.0433  0.0604  0.0449  0.0597  0.0355  0.0531  0.0343  0.1052   \n",
      "15   0.0298  0.0615  0.0650  0.0921  0.1615  0.2294  0.2176  0.2033  0.1459   \n",
      "16   0.0352  0.0116  0.0191  0.0469  0.0737  0.1185  0.1683  0.1541  0.1466   \n",
      "17   0.0192  0.0607  0.0378  0.0774  0.1388  0.0809  0.0568  0.0219  0.1037   \n",
      "18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n",
      "19   0.0126  0.0149  0.0641  0.1732  0.2565  0.2559  0.2947  0.4110  0.4983   \n",
      "20   0.0473  0.0509  0.0819  0.1252  0.1783  0.3070  0.3008  0.2362  0.3830   \n",
      "21   0.0664  0.0575  0.0842  0.0372  0.0458  0.0771  0.0771  0.1130  0.2353   \n",
      "22   0.0099  0.0484  0.0299  0.0297  0.0652  0.1077  0.2363  0.2385  0.0075   \n",
      "23   0.0115  0.0150  0.0136  0.0076  0.0211  0.1058  0.1023  0.0440  0.0931   \n",
      "24   0.0293  0.0644  0.0390  0.0173  0.0476  0.0816  0.0993  0.0315  0.0736   \n",
      "25   0.0201  0.0026  0.0138  0.0062  0.0133  0.0151  0.0541  0.0210  0.0505   \n",
      "26   0.0151  0.0320  0.0599  0.1050  0.1163  0.1734  0.1679  0.1119  0.0889   \n",
      "27   0.0177  0.0300  0.0288  0.0394  0.0630  0.0526  0.0688  0.0633  0.0624   \n",
      "28   0.0100  0.0275  0.0190  0.0371  0.0416  0.0201  0.0314  0.0651  0.1896   \n",
      "29   0.0189  0.0308  0.0197  0.0622  0.0080  0.0789  0.1440  0.1451  0.1789   \n",
      "..      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "178  0.0197  0.0394  0.0384  0.0076  0.0251  0.0629  0.0747  0.0578  0.1357   \n",
      "179  0.0394  0.0420  0.0446  0.0551  0.0597  0.1416  0.0956  0.0802  0.1618   \n",
      "180  0.0310  0.0221  0.0433  0.0191  0.0964  0.1827  0.1106  0.1702  0.2804   \n",
      "181  0.0423  0.0321  0.0709  0.0108  0.1070  0.0973  0.0961  0.1323  0.2462   \n",
      "182  0.0095  0.0308  0.0539  0.0411  0.0613  0.1039  0.1016  0.1394  0.2592   \n",
      "183  0.0096  0.0404  0.0682  0.0688  0.0887  0.0932  0.0955  0.2140  0.2546   \n",
      "184  0.0269  0.0383  0.0505  0.0707  0.1313  0.2103  0.2263  0.2524  0.3595   \n",
      "185  0.0340  0.0625  0.0381  0.0257  0.0441  0.1027  0.1287  0.1850  0.2647   \n",
      "186  0.0209  0.0191  0.0411  0.0321  0.0698  0.1579  0.1438  0.1402  0.3048   \n",
      "187  0.0368  0.0279  0.0103  0.0566  0.0759  0.0679  0.0970  0.1473  0.2164   \n",
      "188  0.0089  0.0274  0.0248  0.0237  0.0224  0.0845  0.1488  0.1224  0.1569   \n",
      "189  0.0158  0.0239  0.0150  0.0494  0.0988  0.1425  0.1463  0.1219  0.1697   \n",
      "190  0.0156  0.0210  0.0282  0.0596  0.0462  0.0779  0.1365  0.0780  0.1038   \n",
      "191  0.0315  0.0252  0.0167  0.0479  0.0902  0.1057  0.1024  0.1209  0.1241   \n",
      "192  0.0056  0.0267  0.0221  0.0561  0.0936  0.1146  0.0706  0.0996  0.1673   \n",
      "193  0.0203  0.0121  0.0380  0.0128  0.0537  0.0874  0.1021  0.0852  0.1136   \n",
      "194  0.0392  0.0108  0.0267  0.0257  0.0410  0.0491  0.1053  0.1690  0.2105   \n",
      "195  0.0129  0.0141  0.0309  0.0375  0.0767  0.0787  0.0662  0.1108  0.1777   \n",
      "196  0.0050  0.0017  0.0270  0.0450  0.0958  0.0830  0.0879  0.1220  0.1977   \n",
      "197  0.0366  0.0421  0.0504  0.0250  0.0596  0.0252  0.0958  0.0991  0.1419   \n",
      "198  0.0238  0.0318  0.0422  0.0399  0.0788  0.0766  0.0881  0.1143  0.1594   \n",
      "199  0.0116  0.0744  0.0367  0.0225  0.0076  0.0545  0.1110  0.1069  0.1708   \n",
      "200  0.0131  0.0387  0.0329  0.0078  0.0721  0.1341  0.1626  0.1902  0.2610   \n",
      "201  0.0335  0.0258  0.0398  0.0570  0.0529  0.1091  0.1709  0.1684  0.1865   \n",
      "202  0.0272  0.0378  0.0488  0.0848  0.1127  0.1103  0.1349  0.2337  0.3113   \n",
      "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n",
      "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  0.1018  0.1030   \n",
      "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n",
      "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  0.1123  0.1945   \n",
      "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  0.1400  0.1843   \n",
      "\n",
      "         9  ...      51      52      53      54      55      56      57  \\\n",
      "0    0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
      "1    0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
      "2    0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
      "3    0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
      "4    0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
      "5    0.3039 ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n",
      "6    0.3513 ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143   \n",
      "7    0.2838 ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n",
      "8    0.1487 ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093   \n",
      "9    0.0251 ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035   \n",
      "10   0.0452 ...  0.0062  0.0120  0.0052  0.0056  0.0093  0.0042  0.0003   \n",
      "11   0.0835 ...  0.0133  0.0265  0.0224  0.0074  0.0118  0.0026  0.0092   \n",
      "12   0.1240 ...  0.0176  0.0127  0.0088  0.0098  0.0019  0.0059  0.0058   \n",
      "13   0.1895 ...  0.0059  0.0095  0.0194  0.0080  0.0152  0.0158  0.0053   \n",
      "14   0.2120 ...  0.0083  0.0057  0.0174  0.0188  0.0054  0.0114  0.0196   \n",
      "15   0.0852 ...  0.0031  0.0153  0.0071  0.0212  0.0076  0.0152  0.0049   \n",
      "16   0.2912 ...  0.0346  0.0158  0.0154  0.0109  0.0048  0.0095  0.0015   \n",
      "17   0.1186 ...  0.0331  0.0131  0.0120  0.0108  0.0024  0.0045  0.0037   \n",
      "18   0.1520 ...  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120  0.0132   \n",
      "19   0.5920 ...  0.0092  0.0035  0.0098  0.0121  0.0006  0.0181  0.0094   \n",
      "20   0.3759 ...  0.0193  0.0118  0.0064  0.0042  0.0054  0.0049  0.0082   \n",
      "21   0.1838 ...  0.0141  0.0190  0.0043  0.0036  0.0026  0.0024  0.0162   \n",
      "22   0.1882 ...  0.0173  0.0149  0.0115  0.0202  0.0139  0.0029  0.0160   \n",
      "23   0.0734 ...  0.0091  0.0016  0.0084  0.0064  0.0026  0.0029  0.0037   \n",
      "24   0.0860 ...  0.0035  0.0052  0.0083  0.0078  0.0075  0.0105  0.0160   \n",
      "25   0.1097 ...  0.0108  0.0070  0.0063  0.0030  0.0011  0.0007  0.0024   \n",
      "26   0.1205 ...  0.0061  0.0015  0.0084  0.0128  0.0054  0.0011  0.0019   \n",
      "27   0.0613 ...  0.0102  0.0122  0.0044  0.0075  0.0124  0.0099  0.0057   \n",
      "28   0.2668 ...  0.0088  0.0104  0.0036  0.0088  0.0047  0.0117  0.0020   \n",
      "29   0.2522 ...  0.0038  0.0096  0.0142  0.0190  0.0140  0.0099  0.0092   \n",
      "..      ... ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "178  0.1695 ...  0.0134  0.0097  0.0042  0.0058  0.0072  0.0041  0.0045   \n",
      "179  0.2558 ...  0.0146  0.0040  0.0114  0.0032  0.0062  0.0101  0.0068   \n",
      "180  0.4432 ...  0.0204  0.0059  0.0053  0.0079  0.0037  0.0015  0.0056   \n",
      "181  0.2696 ...  0.0176  0.0035  0.0093  0.0121  0.0075  0.0056  0.0021   \n",
      "182  0.3745 ...  0.0181  0.0019  0.0102  0.0133  0.0040  0.0042  0.0030   \n",
      "183  0.2952 ...  0.0237  0.0078  0.0144  0.0170  0.0012  0.0109  0.0036   \n",
      "184  0.5915 ...  0.0167  0.0199  0.0145  0.0081  0.0045  0.0043  0.0027   \n",
      "185  0.4117 ...  0.0141  0.0019  0.0067  0.0099  0.0042  0.0057  0.0051   \n",
      "186  0.3914 ...  0.0078  0.0201  0.0104  0.0039  0.0031  0.0062  0.0087   \n",
      "187  0.2544 ...  0.0105  0.0024  0.0018  0.0057  0.0092  0.0009  0.0086   \n",
      "188  0.2119 ...  0.0096  0.0103  0.0093  0.0025  0.0044  0.0021  0.0069   \n",
      "189  0.1923 ...  0.0121  0.0108  0.0057  0.0028  0.0079  0.0034  0.0046   \n",
      "190  0.1567 ...  0.0150  0.0060  0.0082  0.0091  0.0038  0.0056  0.0056   \n",
      "191  0.1533 ...  0.0108  0.0062  0.0044  0.0072  0.0007  0.0054  0.0035   \n",
      "192  0.1859 ...  0.0072  0.0055  0.0074  0.0068  0.0084  0.0037  0.0024   \n",
      "193  0.1747 ...  0.0134  0.0094  0.0047  0.0045  0.0042  0.0028  0.0036   \n",
      "194  0.2471 ...  0.0083  0.0080  0.0026  0.0079  0.0042  0.0071  0.0044   \n",
      "195  0.2245 ...  0.0124  0.0093  0.0072  0.0019  0.0027  0.0054  0.0017   \n",
      "196  0.2282 ...  0.0165  0.0056  0.0010  0.0027  0.0062  0.0024  0.0063   \n",
      "197  0.1847 ...  0.0132  0.0027  0.0022  0.0059  0.0016  0.0025  0.0017   \n",
      "198  0.2048 ...  0.0096  0.0071  0.0084  0.0038  0.0026  0.0028  0.0013   \n",
      "199  0.2271 ...  0.0141  0.0103  0.0100  0.0034  0.0026  0.0037  0.0044   \n",
      "200  0.3193 ...  0.0150  0.0076  0.0032  0.0037  0.0071  0.0040  0.0009   \n",
      "201  0.2660 ...  0.0120  0.0039  0.0053  0.0062  0.0046  0.0045  0.0022   \n",
      "202  0.3997 ...  0.0091  0.0045  0.0043  0.0043  0.0098  0.0054  0.0051   \n",
      "203  0.2684 ...  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065  0.0115   \n",
      "204  0.2154 ...  0.0061  0.0093  0.0135  0.0063  0.0063  0.0034  0.0032   \n",
      "205  0.2529 ...  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140  0.0138   \n",
      "206  0.2354 ...  0.0086  0.0046  0.0126  0.0036  0.0035  0.0034  0.0079   \n",
      "207  0.2354 ...  0.0146  0.0129  0.0047  0.0039  0.0061  0.0040  0.0036   \n",
      "\n",
      "         58      59  60  \n",
      "0    0.0090  0.0032   R  \n",
      "1    0.0052  0.0044   R  \n",
      "2    0.0095  0.0078   R  \n",
      "3    0.0040  0.0117   R  \n",
      "4    0.0107  0.0094   R  \n",
      "5    0.0051  0.0062   R  \n",
      "6    0.0036  0.0103   R  \n",
      "7    0.0048  0.0053   R  \n",
      "8    0.0059  0.0022   R  \n",
      "9    0.0056  0.0040   R  \n",
      "10   0.0053  0.0036   R  \n",
      "11   0.0009  0.0044   R  \n",
      "12   0.0059  0.0032   R  \n",
      "13   0.0189  0.0102   R  \n",
      "14   0.0147  0.0062   R  \n",
      "15   0.0200  0.0073   R  \n",
      "16   0.0073  0.0067   R  \n",
      "17   0.0112  0.0075   R  \n",
      "18   0.0070  0.0088   R  \n",
      "19   0.0116  0.0063   R  \n",
      "20   0.0028  0.0027   R  \n",
      "21   0.0109  0.0079   R  \n",
      "22   0.0106  0.0134   R  \n",
      "23   0.0070  0.0041   R  \n",
      "24   0.0095  0.0011   R  \n",
      "25   0.0057  0.0044   R  \n",
      "26   0.0023  0.0062   R  \n",
      "27   0.0032  0.0019   R  \n",
      "28   0.0091  0.0058   R  \n",
      "29   0.0052  0.0075   R  \n",
      "..      ...     ...  ..  \n",
      "178  0.0047  0.0054   M  \n",
      "179  0.0053  0.0087   M  \n",
      "180  0.0067  0.0054   M  \n",
      "181  0.0043  0.0017   M  \n",
      "182  0.0031  0.0033   M  \n",
      "183  0.0043  0.0018   M  \n",
      "184  0.0055  0.0057   M  \n",
      "185  0.0033  0.0058   M  \n",
      "186  0.0070  0.0042   M  \n",
      "187  0.0110  0.0052   M  \n",
      "188  0.0060  0.0018   M  \n",
      "189  0.0022  0.0021   M  \n",
      "190  0.0048  0.0024   M  \n",
      "191  0.0001  0.0055   M  \n",
      "192  0.0034  0.0007   M  \n",
      "193  0.0013  0.0016   M  \n",
      "194  0.0022  0.0014   M  \n",
      "195  0.0024  0.0029   M  \n",
      "196  0.0017  0.0028   M  \n",
      "197  0.0027  0.0027   M  \n",
      "198  0.0035  0.0060   M  \n",
      "199  0.0057  0.0035   M  \n",
      "200  0.0015  0.0085   M  \n",
      "201  0.0005  0.0031   M  \n",
      "202  0.0065  0.0103   M  \n",
      "203  0.0193  0.0157   M  \n",
      "204  0.0062  0.0067   M  \n",
      "205  0.0077  0.0031   M  \n",
      "206  0.0036  0.0048   M  \n",
      "207  0.0061  0.0115   M  \n",
      "\n",
      "[208 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "f = 5\n",
    "label_ind=60\n",
    "x = 3\n",
    "print(df[f][x], df[label_ind][x])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlreadyFitException(Exception):\n",
    "    pass\n",
    "class NoBreakpointsException(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:06:45.776850Z",
     "start_time": "2017-10-17T17:06:45.570177Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A dummy version of tree nodes\n",
    "'''\n",
    "class Node:\n",
    "      \n",
    "    def __init__(self, data, rows, features, depth, max_depth):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.data = data\n",
    "        self.rows = rows\n",
    "        self.features = features\n",
    "        self.label_index = 60\n",
    "        self.labels = ['R', 'M']\n",
    "        self.spliting_feature_val = None\n",
    "        self.id = '%030x' % random.randrange(16**30)\n",
    "        self.depth = depth\n",
    "        self.max_depth = max_depth\n",
    "        self.min_feature = None\n",
    "        self.min_break_point = None\n",
    "        self.min_gini = None\n",
    "\n",
    "    \n",
    "    def calc_shannon_entropy(self):\n",
    "        raw_val = 0\n",
    "        for label in self.labels:\n",
    "            members = self.data.loc[self.data[self.label_index] == label]\n",
    "            if len(members) <= 0: continue\n",
    "            filtered = [x for x in members.index.values if x in self.rows]\n",
    "            intermediate = len(filtered)/len(self.rows)\n",
    "            raw_val += -intermediate*np.log2(intermediate)\n",
    "        return raw_val\n",
    "    \n",
    "    def calc_gini_index(self):\n",
    "        raw_val = 1\n",
    "        members = [self.data[self.label_index][x] for x in self.rows]\n",
    "        for label in self.labels:\n",
    "#             members = self.data.loc[self.data[self.label_index] == label]\n",
    "            #maybe do as a for loop?\n",
    "            filtered = [x for x in members if x == label]\n",
    "#             filtered = members\n",
    "            raw_val -= (len(filtered)/len(self.rows))**2\n",
    "        return raw_val\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    calculate info gain from gini/entropy\n",
    "    '''\n",
    "    def cal_info_gain():\n",
    "        pass\n",
    "    \n",
    "    def find_break_points(self, df, feature):\n",
    "        breaks = []\n",
    "        for i in range(len(df)-1):\n",
    "            row = df[i:i+1]\n",
    "            next_row = df[i+1:i+2]\n",
    "#             print(row[self.label_index])\n",
    "            if row[self.label_index].values[0] != next_row[self.label_index].values[0]:\n",
    "                breaks.append(next_row[feature].values[0]) #float precision issue, care\n",
    "        return breaks\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    Choose the best feature to split at this point\n",
    "    i.e. low gini/entropy, high infoGain\n",
    "    '''\n",
    "    \n",
    "    def split(self):\n",
    "        #are we a leaf node?\n",
    "        if len(self.rows) == 0:\n",
    "            raise ValueError('The node has no document feed, no more splitting')\n",
    "        elif self.calc_gini_index() == 0:\n",
    "            raise ValueError('The node is pure, no more splitting')\n",
    "        elif self.depth == self.max_depth:\n",
    "            raise ValueError('The node has reached max recursion depth, no more splitting')\n",
    "        elif len(self.features) == 0:\n",
    "            raise ValueError('There are no more features to split on.')\n",
    "            \n",
    "        #we are not a leaf node.\n",
    "        min_gini, min_feature, min_break_point, left_members, right_members = 2, -999, -999, [], []\n",
    "        bp_len_sum = 0\n",
    "        for feature in self.features:\n",
    "#             print('parsing')\n",
    "            to_parse = [(self.data[feature][x],self.data[self.label_index][x]) for x in self.rows]\n",
    "            to_parse = pd.DataFrame(to_parse, columns=(feature,self.label_index), index=self.rows)\n",
    "#             print(to_parse)\n",
    "            to_parse.sort_values(feature, inplace=True)\n",
    "#             print(to_parse)\n",
    "#             to_parse = self.data[[feature, self.label_index]]\n",
    "#             to_parse = to_parse.loc[to_parse.index.isin (self.rows)]\n",
    "#             to_parse.sort_values(feature, inplace=True)\n",
    "#             print(to_parse)\n",
    "            break_points = self.find_break_points(to_parse, feature)\n",
    "#             print(break_points)\n",
    "            bp_len_sum += len(break_points)\n",
    "\n",
    "            best_gini_this_feature, best_breakpoint_this_feature = self.find_best_breakpoint(to_parse.values[:,0], to_parse.values[:,1])\n",
    "            if best_gini_this_feature < min_gini:\n",
    "                left_members = to_parse.loc[to_parse[feature] < best_breakpoint_this_feature].index.values\n",
    "                right_members = to_parse.loc[to_parse[feature] >= best_breakpoint_this_feature].index.values\n",
    "                min_gini, min_break_point, min_feature = best_gini_this_feature, best_breakpoint_this_feature, feature\n",
    "        if bp_len_sum == 0:\n",
    "            print(to_parse)\n",
    "            print(self.calc_gini_index())\n",
    "        #Node(self.data, , [x for x in self.features if x != feature], self.depth+1, self.max_depth)\n",
    "        self.left = Node(self.data, left_members, [x for x in self.features if x != min_feature], self.depth+1, self.max_depth)\n",
    "        self.right = Node(self.data, right_members, [x for x in self.features if x != min_feature], self.depth+1, self.max_depth)\n",
    "        self.min_feature, self.min_break_point, self.min_gini = min_feature, min_break_point, min_gini\n",
    "        try:\n",
    "            if self.left is None:\n",
    "                print(self.min_feature,self.min_break_point,self.min_gini)\n",
    "            self.left.split()\n",
    "        except ValueError: # probably need a customized error class\n",
    "            pass\n",
    "        try:\n",
    "            self.right.split()\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    '''\n",
    "    A faster way to find the best breakpoint. \n",
    "    Note that we're assuming that the node we're splitting isn't pure.\n",
    "    \n",
    "    input:\n",
    "    values - an arraylike of the values associated with each element\n",
    "    classes - a arraylike of the labels for each element, in the same order as values\n",
    "    \n",
    "    returns:\n",
    "    (min_gini, min_break_point)\n",
    "    '''\n",
    "    def find_best_breakpoint(self, values, classes):\n",
    "        if len(values) != len(classes):\n",
    "            raise ValueError(\"Values and classes must be the same length.\")\n",
    "        best_gini = 2\n",
    "        best_ind = -1\n",
    "        #class member values\n",
    "        left_members = {}\n",
    "        right_members = {}\n",
    "        \n",
    "        #everything starts on the right\n",
    "        for i in range(len(values)):\n",
    "            try:\n",
    "                right_members[classes[i]] += 1\n",
    "            except KeyError:\n",
    "                right_members[classes[i]] = 1\n",
    "                \n",
    "        #compare different breakpoints\n",
    "        for i in range(len(values)-1):\n",
    "            #add ith value to the left (we're considering splitting after i)\n",
    "            try:\n",
    "                left_members[classes[i]] += 1\n",
    "            except KeyError:\n",
    "                left_members[classes[i]] = 1\n",
    "            \n",
    "            #remove ith value from the right \n",
    "            right_members[classes[i]] -= 1\n",
    "            \n",
    "            #if i and i+1 aren't the same class, consider splitting here\n",
    "            if classes[i] != classes[i+1]:\n",
    "                left_gini = Node.calc_gini_from_props(left_members)\n",
    "                right_gini = Node.calc_gini_from_props(right_members)\n",
    "                curr_gini = Node.aggregate_gini(left_gini, right_gini, i+1, len(values)-(i+1))\n",
    "                if best_gini > curr_gini:\n",
    "                    best_gini = curr_gini\n",
    "                    best_ind = i+1 #if we're less than the breakpoint, we're put in one bucket, and geq is in the other bucket\n",
    "        #return the best value\n",
    "        return (best_gini, values[best_ind])\n",
    "            \n",
    "    '''\n",
    "    Calculates the gini index from a dictionary of proportions\n",
    "    \n",
    "    input:\n",
    "    members - a dict from string label to int count of members\n",
    "    \n",
    "    returns - the gini index for a node containing these members\n",
    "    '''        \n",
    "    def calc_gini_from_props(members):\n",
    "        answer = 1\n",
    "        total = 0\n",
    "        for label in members.keys():\n",
    "            total += members[label]\n",
    "        for label in members.keys():\n",
    "            answer -= (members[label]/total)**2\n",
    "        return answer\n",
    "    \n",
    "    '''\n",
    "    Calculates the aggregate gini index from two child nodes.\n",
    "    \n",
    "    input:\n",
    "    score1 - the gini score for one child\n",
    "    score2 - the gini score for the other child\n",
    "    num1 - the number of members of the first child\n",
    "    num2 - the number of members of the second child\n",
    "    \n",
    "    returns - a weighted average of the two scores\n",
    "    '''\n",
    "    def aggregate_gini(score1, score2, num1, num2):\n",
    "        return (score1*num1 + score2*num2)/(num1 + num2)\n",
    "    \n",
    "    def __str__(self):\n",
    "#         if self.left and self.right:\n",
    "        children = [x.id for x in (self.left, self.right)] if self.left and self.right else []\n",
    "        return \"[{ID}, {Gini}, {Size}, {Feature}, {BP}, {Children}]\".format(ID=self.id, \n",
    "                                                            Gini = self.calc_gini_index(),\n",
    "                                                            Size = len(self.rows),\n",
    "                                                            Feature=self.min_feature, \n",
    "                                                            BP=self.min_break_point,\n",
    "                                                           Children=children)\n",
    "#         else:\n",
    "#             \"[{ID}, (Children=None)]\".format(ID=self.id)\n",
    "    \n",
    "    def get_proportions(self, target_label):\n",
    "        members = [self.data[self.label_index][x] for x in self.rows]\n",
    "        filtered = [x for x in members if x == target_label]\n",
    "#         members = self.data.loc[self.data[self.label_index] == target_label]\n",
    "#         filtered = [x for x in members.index.values if x in self.rows]\n",
    "        raw_val = (len(filtered)/len(self.rows))\n",
    "        return raw_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-17T17:07:21.996457Z",
     "start_time": "2017-10-17T17:06:46.020532Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls = [x for x in range(60)]\n",
    "dummy = Node(df, [1,2,2,7,4,5,200,150, 175, 175, 130], ls, 0, 2)\n",
    "dummy.split()\n",
    "# print(dummy.left.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4576a9d8bb6c52b66d3921f7edc879, 0.375, 20, 40, 0.1151, ['b8d33d76d47e31638fb41faef81075', '51c1b4bccd81cb3cfc035cf537d39a']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[b8d33d76d47e31638fb41faef81075, 0.31999999999999984, 5, 2, 0.0347, ['7b0156ed9c5645c8a94d8fe1625568', '110be92be7cbe80337780f120c6028']]\n",
      "[51c1b4bccd81cb3cfc035cf537d39a, 0.12444444444444439, 15, 2, 0.0152, ['05e6533a9bb4a6198ea7ceba8ff755', '7ac7d45615cdff06082edff36c06f8']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[7b0156ed9c5645c8a94d8fe1625568, 0.0, 4, None, None, []]\n",
      "[110be92be7cbe80337780f120c6028, 0.0, 1, None, None, []]\n",
      "[05e6533a9bb4a6198ea7ceba8ff755, 0.0, 1, None, None, []]\n",
      "[7ac7d45615cdff06082edff36c06f8, 0.0, 14, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nodes = [dummy]\n",
    "while(len(nodes) > 0):\n",
    "    new_nodes = []\n",
    "    level_str = ''\n",
    "    for node in nodes:\n",
    "        level_str += str(node) + \"\\n\"\n",
    "        if node.left:\n",
    "            new_nodes.append(node.left)\n",
    "        if node.right:\n",
    "            new_nodes.append(node.right)\n",
    "    print(level_str+\"\\n--------------------------------------------------\", end='\\n')\n",
    "    nodes = new_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T03:09:58.069975Z",
     "start_time": "2017-10-15T03:09:58.052936Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A dummy implementation of decision trees\n",
    "'''\n",
    "class Tree:\n",
    "    \n",
    "    '''\n",
    "    params:\n",
    "    train_data - training data to trainthe tree\n",
    "    depth - max recursion depth of the tree\n",
    "    benchmark - benchmark for geni/entropy\n",
    "    '''\n",
    "    def __init__(self, data, depth, benchmark, rows, features): #should we include data here\n",
    "        self.depth = depth\n",
    "        self.rows = rows\n",
    "        self.features = features\n",
    "        self.data = data\n",
    "        self.benchmark = benchmark\n",
    "        self.head = Node(data, rows, features, 0, depth)\n",
    "        self.oob_error = -1\n",
    "        \n",
    "    '''\n",
    "    Recursively split until geni/entropy benchmark met or max_depth reached\n",
    "    '''\n",
    "    def fit(self):\n",
    "        #think about behavior of pure nodes more\n",
    "        try:\n",
    "            self.head.split()\n",
    "        except ValueError: #change this to whatever node.split() throws\n",
    "            print('Head is a pure node.')\n",
    "    '''\n",
    "    params: \n",
    "    test_data - test data to run the prediction on\n",
    "    \n",
    "    return: \n",
    "    outputs confidence/probability of each category\n",
    "    '''\n",
    "    def predict(self, test_data):\n",
    "#         assuming input data is a dataframe right now\n",
    "        cur_node = self.head\n",
    "        while (cur_node.left and cur_node.right):\n",
    "            if (test_data[cur_node.min_feature].values[0] < cur_node.min_break_point):\n",
    "                cur_node = cur_node.left\n",
    "            else:\n",
    "                cur_node = cur_node.right\n",
    "        \n",
    "#         here, cur_node should be the leaf\n",
    "        r_confidence = cur_node.get_proportions('R')\n",
    "        m_confidence = cur_node.get_proportions('M')\n",
    "        \n",
    "        return (r_confidence, m_confidence)\n",
    "    \n",
    "    '''\n",
    "    params: \n",
    "    more_data - more training data to update the tree\n",
    "    \n",
    "    return: \n",
    "    Null or we can say something like which nodes are changed\n",
    "    '''\n",
    "    def update(more_data):\n",
    "        #decide whether to call alg 3 or alg 4\n",
    "        #call the relevant one\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    return:\n",
    "    The number of ignored data pieces that we get incorrect (n) divided by the number of rows we ignored (l)\n",
    "    That is, n/l\n",
    "    '''\n",
    "    def calc_oob_error(self):\n",
    "        #complement of rows\n",
    "        test_data = self.data.loc[~self.data.index.isin(self.rows)]\n",
    "        complement = set(range(self.data.shape[1])) - set(self.rows)\n",
    "        #predict each of those (TODO: update this once we have batch training)\n",
    "        num_incorrect = 0\n",
    "        for row in complement:\n",
    "            case = self.data.loc[[row]]\n",
    "            prediction = self.predict(case)\n",
    "            if prediction[0] > prediction[1]:\n",
    "                num_incorrect += 1 if case[60].values[0] == 'M' else 0\n",
    "            else:\n",
    "                num_incorrect += 1 if case[60].values[0] == 'R' else 0\n",
    "        return num_incorrect / len(test_data)\n",
    "        #calculate incorrect / total\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    Maybe we can use pickle for this\n",
    "    '''\n",
    "    def store_tree(file_path):\n",
    "        pass\n",
    "    \n",
    "    def load_tree(file_path):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    String representation\n",
    "    '''\n",
    "    def __str__(self):\n",
    "        string = ''\n",
    "        string += str(sorted(self.features))\n",
    "        string += '\\n'\n",
    "        nodes = [self.head]\n",
    "        while(len(nodes) > 0):\n",
    "            new_nodes = []\n",
    "            level_str = ''\n",
    "            for node in nodes:\n",
    "                level_str += str(node) + \"\\n\"\n",
    "                if node.left:\n",
    "                    new_nodes.append(node.left)\n",
    "                if node.right:\n",
    "                    new_nodes.append(node.right)\n",
    "            string += level_str+\"\\n--------------------------------------------------\\n\"\n",
    "            nodes = new_nodes\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "ls = [x for x in range(60)]\n",
    "# dummy = Node(df, range(df.shape[0]-20), ls, 0, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val(df, tries):\n",
    "    for i in range(tries):\n",
    "        shuffle = df.sample(frac=1)\n",
    "        tree = Tree(df, 3, None, range(shuffle.shape[0]-20), ls)\n",
    "        tree.fit()\n",
    "        score = 0\n",
    "        for i in range(188, 208):\n",
    "            actual = shuffle[i:i+1][60].values[0]\n",
    "            p = tree.predict(shuffle[i:i+1])\n",
    "            if p[0] > p[1]:\n",
    "        #         print('R/{}'.format(actual))\n",
    "                if 'R' == actual: \n",
    "                    score+=1\n",
    "            else:\n",
    "        #         print('M/{}'.format(actual))\n",
    "                if 'M' == actual: \n",
    "                    score+=1\n",
    "        print(score/(208-188))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.95\n",
      "0.95\n",
      "0.85\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "cross_val(df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-15T03:09:58.617626Z",
     "start_time": "2017-10-15T03:09:58.578957Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Dummy Version of Random Forest\n",
    "'''\n",
    "class RNF: \n",
    "    '''\n",
    "    params:\n",
    "    train_data - training data to trainthe tree\n",
    "    n_trees - number of trees to setup\n",
    "    tree_depth - max recursive\n",
    "    random_seed - seed for random gen\n",
    "    n_max_features - max num of features to pass to each tree\n",
    "    n_max_input - max num of input to pass to each tree\n",
    "    '''\n",
    "    def __init__(self, train_data, n_trees, tree_depth, random_seed, n_max_features, n_max_input):\n",
    "        self.trees = []\n",
    "        self.train_data = train_data\n",
    "        self.n_trees = n_trees\n",
    "        self.tree_depth = tree_depth\n",
    "        self.n_max_features = n_max_features\n",
    "        self.n_max_input = n_max_input\n",
    "#         self.features = [()] #list of tuples like (tree, emails, features)\n",
    "        random.seed(random_seed)\n",
    "    \n",
    "        np.random.seed(random_seed)\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    Randomly select features and emails from the train_data \n",
    "    '''\n",
    "    def random_select(self, train_data):\n",
    "        selected_rows = np.random.choice(self.train_data.shape[0], self.n_max_input)\n",
    "        selected_features = np.random.choice(self.train_data.shape[1] - 1, self.n_max_features, replace=False)\n",
    "        return (selected_rows, selected_features)\n",
    "        \n",
    "    '''\n",
    "    pass randomly selected emails and features to each tree\n",
    "    '''\n",
    "    def fit(self):\n",
    "        if len(self.trees) != 0:\n",
    "            raise AlreadyFitException('This forest has already been fit to the data')\n",
    "        for i in range(self.n_trees):\n",
    "            selected = self.random_select(self.train_data)\n",
    "#             self, train_data, depth, benchmark, rows, features\n",
    "            self.trees.append(Tree(self.train_data, self.tree_depth, 0, selected[0], selected[1]))\n",
    "        for tree in self.trees:\n",
    "            tree.fit()\n",
    "    \n",
    "    '''\n",
    "    calculate a proba from output of each tree's prediction\n",
    "    should ouput two arrays: probas and classfication\n",
    "    '''\n",
    "    def some_majority_count_metric(self, scores):\n",
    "        return np.mean(scores, axis=0)\n",
    "    \n",
    "    def predict(self, test_data):\n",
    "        scores = [tree.predict(test_data) for tree in self.trees]\n",
    "        probas = self.some_majority_count_metric(scores)\n",
    "        classes = 'R' if probas[0] > probas[1] else 'M'\n",
    "        return self.some_majority_count_metric(scores), classes\n",
    "    \n",
    "    '''\n",
    "    params: \n",
    "    more_data - more training data to update the forest\n",
    "    \n",
    "    return: \n",
    "    Null or we can say something like which trees are changed\n",
    "    '''\n",
    "    def update(more_data):\n",
    "        #add more_data to the end of self.train_data\n",
    "        \n",
    "        #calc oob error for each tree\n",
    "        \n",
    "        #calc threshold\n",
    "        \n",
    "        #for each tree in trees:\n",
    "        #if oob < thresh\n",
    "            #alg 3 (trash the tree and build a new one)\n",
    "        #else alg 4\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "    Maybe we can use pickle for this\n",
    "    '''\n",
    "    def store_rnf(file_path):\n",
    "        pass\n",
    "    \n",
    "    def load_rnf(file_path):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = RNF(df[:188], 10, 2, 42, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 9, 12, 16, 22, 30, 40, 48, 59]\n",
      "[477183392456de3eb13b9046685257, 0.5, 10, 48, 0.0588, ['961b768d5288f1142c3fe860e7a113', '6273a693cd59bf5c941cf0dc98d2c1']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[961b768d5288f1142c3fe860e7a113, 0.2777777777777777, 6, 59, 0.0092, ['e8256547294739614ff3d719db3ad0', '5347655d65a441d58842dea2bc372f']]\n",
      "[6273a693cd59bf5c941cf0dc98d2c1, 0.0, 4, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "[e8256547294739614ff3d719db3ad0, 0.0, 5, None, None, []]\n",
      "[5347655d65a441d58842dea2bc372f, 0.0, 1, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[3, 5, 12, 21, 26, 30, 33, 41, 42, 54]\n",
      "[2ff8d207a0ca6e0822e8f36c031199, 0.31999999999999984, 10, 41, 0.4008, ['7d57dbbaa80dd488bd64072bcfbe01', '8a36996123fdf77656af7229d4beef']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[7d57dbbaa80dd488bd64072bcfbe01, 0.0, 7, None, None, []]\n",
      "[8a36996123fdf77656af7229d4beef, 0.4444444444444444, 3, 54, 0.0031, ['a60862af42e12f3838b3268e944239', '1ca3e6c6a7ee39c4b032ccd7c524a5']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[a60862af42e12f3838b3268e944239, 0.0, 1, None, None, []]\n",
      "[1ca3e6c6a7ee39c4b032ccd7c524a5, 0.0, 2, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[3, 15, 17, 22, 34, 36, 39, 42, 46, 52]\n",
      "[d6cb4d8b8148f6b38a088ca65ed389, 0.48, 10, 34, 0.7505, ['21e379448aaa9e66b2bc5b50c187fc', '6cdd62508ebad7b7c93acfe059a0ee']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[21e379448aaa9e66b2bc5b50c187fc, 0.24489795918367355, 7, 52, 0.0028, ['492677757750a9a491f0b2ea1fca65', 'fcae1b827050a82369b584ff5e9ff0']]\n",
      "[6cdd62508ebad7b7c93acfe059a0ee, 0.0, 3, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "[492677757750a9a491f0b2ea1fca65, 0.0, 1, None, None, []]\n",
      "[fcae1b827050a82369b584ff5e9ff0, 0.0, 6, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[3, 14, 20, 26, 40, 43, 45, 46, 54, 56]\n",
      "[8e6f0396da1dac72ff5d2a386ecbe0, 0.31999999999999984, 10, 3, 0.0411, ['51e929a0a04dc427209bdf1c11f735', '80b959877409a977d21e02ff01cf99']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[51e929a0a04dc427209bdf1c11f735, 0.0, 2, None, None, []]\n",
      "[80b959877409a977d21e02ff01cf99, 0.0, 8, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[9, 10, 12, 22, 25, 30, 33, 37, 49, 51]\n",
      "[d860eab2b9437a28df6ec4ce4a2bbd, 0.31999999999999984, 10, 33, 0.2392, ['3aa686b88139b9ae270da702f06b90', 'ae2b7aa4161293c4c2e2e3444ea7c8']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[3aa686b88139b9ae270da702f06b90, 0.0, 1, None, None, []]\n",
      "[ae2b7aa4161293c4c2e2e3444ea7c8, 0.19753086419753085, 9, 51, 0.0045, ['50fa0d6f4cc69a4b22d3081c8eaee9', 'bf71a22720797d32ebd6899be578c7']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[50fa0d6f4cc69a4b22d3081c8eaee9, 0.0, 1, None, None, []]\n",
      "[bf71a22720797d32ebd6899be578c7, 0.0, 8, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[2, 3, 20, 29, 35, 36, 40, 44, 50, 55]\n",
      "[6e3d9a27cd813047229389571aa876, 0.48, 10, 3, 0.0386, ['fa2a8752fbe43b99546eb400257ad1', 'b9d89dedd968311ca35cfb04fc6d82']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[fa2a8752fbe43b99546eb400257ad1, 0.0, 5, None, None, []]\n",
      "[b9d89dedd968311ca35cfb04fc6d82, 0.31999999999999984, 5, 50, 0.036, ['7b53850ed42f1a3d4cbf374eb93eff', '2852d8f26b4776913e4de2e0c53cb8']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[7b53850ed42f1a3d4cbf374eb93eff, 0.0, 4, None, None, []]\n",
      "[2852d8f26b4776913e4de2e0c53cb8, 0.0, 1, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[1, 5, 7, 10, 12, 20, 35, 44, 49, 52]\n",
      "[3454e7562b0f79c37459eef50bea63, 0.4200000000000001, 10, 1, 0.0163, ['548b858cbfedb0f264accc79ac1b1e', 'cc48b74fcca39ab683d2e6337ea2df']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[548b858cbfedb0f264accc79ac1b1e, 0.0, 3, None, None, []]\n",
      "[cc48b74fcca39ab683d2e6337ea2df, 0.0, 7, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[5, 13, 14, 15, 33, 38, 54, 55, 56, 58]\n",
      "[b7cc2518c267976142ea7d17be3111, 0.31999999999999984, 10, 54, 0.0099, ['bf30f8a65e688eabf3ad39fec21bbe', 'e72931847fd9b4e64d1bcb702753a1']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[bf30f8a65e688eabf3ad39fec21bbe, 0.0, 7, None, None, []]\n",
      "[e72931847fd9b4e64d1bcb702753a1, 0.4444444444444444, 3, 15, 0.4335, ['20c8003985c3cf3f76be1d1efa2197', '03ae8438602ab696a402f23ae8cc93']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[20c8003985c3cf3f76be1d1efa2197, 0.0, 2, None, None, []]\n",
      "[03ae8438602ab696a402f23ae8cc93, 0.0, 1, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[0, 3, 19, 26, 31, 32, 37, 38, 44, 58]\n",
      "[876f479a8dca03580d7b71d8f56413, 0.48, 10, 58, 0.0124, ['1e24b3a18ff6b6b535106e122c9a56', '10155be7c99b26114125c63a9bedd4']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[1e24b3a18ff6b6b535106e122c9a56, 0.0, 6, None, None, []]\n",
      "[10155be7c99b26114125c63a9bedd4, 0.0, 4, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "[1, 23, 28, 29, 36, 37, 45, 51, 55, 57]\n",
      "[eb39bcbacfb3d00b1f9163ce9ff57f, 0.48, 10, 1, 0.0453, ['f8883fab4220a7474a493b3ceddf2d', 'f22982c8dcd19f3e3511287900f7f9']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[f8883fab4220a7474a493b3ceddf2d, 0.0, 5, None, None, []]\n",
      "[f22982c8dcd19f3e3511287900f7f9, 0.31999999999999984, 5, 23, 1.0, ['304b7830beb45f683514f2ceb81f9d', 'b5667d6e595ed3a8b317fa18d0752b']]\n",
      "\n",
      "--------------------------------------------------\n",
      "[304b7830beb45f683514f2ceb81f9d, 0.0, 4, None, None, []]\n",
      "[b5667d6e595ed3a8b317fa18d0752b, 0.0, 1, None, None, []]\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tree in a.trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  1.]), 'M')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.predict(df[200:201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0       1       2       3       4       5       6       7      8   \\\n",
      "200  0.0131  0.0387  0.0329  0.0078  0.0721  0.1341  0.1626  0.1902  0.261   \n",
      "\n",
      "         9  ...     51      52      53      54      55     56      57      58  \\\n",
      "200  0.3193 ...  0.015  0.0076  0.0032  0.0037  0.0071  0.004  0.0009  0.0015   \n",
      "\n",
      "         59  60  \n",
      "200  0.0085   M  \n",
      "\n",
      "[1 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[200:201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09497206703910614\n",
      "['M', 'M', 'M', 'M', 'M', 'R', 'R', 'R', 'R', 'R']\n",
      "-----\n",
      "0.05056179775280899\n",
      "['M', 'M', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "-----\n",
      "0.08379888268156424\n",
      "['M', 'M', 'M', 'M', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "-----\n",
      "0.1452513966480447\n",
      "['M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'R', 'R']\n",
      "-----\n",
      "0.20786516853932585\n",
      "['M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'R', 'R']\n",
      "-----\n",
      "0.1452513966480447\n",
      "['M', 'M', 'M', 'M', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "-----\n",
      "0.2247191011235955\n",
      "['M', 'M', 'M', 'M', 'M', 'M', 'M', 'R', 'R', 'R']\n",
      "-----\n",
      "0.07303370786516854\n",
      "['M', 'M', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "-----\n",
      "0.028089887640449437\n",
      "['M', 'M', 'M', 'M', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "-----\n",
      "0.09550561797752809\n",
      "['M', 'M', 'M', 'M', 'R', 'R', 'R', 'R', 'R', 'R']\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for tree in a.trees:\n",
    "    print(tree.calc_oob_error())\n",
    "    print(sorted(list(map(lambda x : df.loc[[x]][60].values[0], tree.rows))))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09497206703910614"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.trees[0].calc_oob_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['M', 'M', 'R', 'R', 'M', 'R', 'R', 'M', 'M', 'R']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x : df.loc[[x]][60].values[0], a.trees[0].rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_val_rnf(df, tries):\n",
    "    for i in range(tries):\n",
    "        shuffle = df.sample(frac=1)\n",
    "        forest = RNF(df[0:188], 50, 4, random.randint(1, 999), 40, 80)\n",
    "        forest.fit()\n",
    "        score = 0\n",
    "        for i in range(188, 208):\n",
    "            actual = shuffle[i:i+1][60].values[0]\n",
    "            p = forest.predict(shuffle[i:i+1])[1]\n",
    "            if p == actual:\n",
    "                score += 1\n",
    "#             if p[0] > p[1]:\n",
    "#         #         print('R/{}'.format(actual))\n",
    "#                 if 'R' == actual: \n",
    "#                     score+=1\n",
    "#             else:\n",
    "#         #         print('M/{}'.format(actual))\n",
    "#                 if 'M' == actual: \n",
    "#                     score+=1\n",
    "        print(score/(208-188))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "cross_val_rnf(df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
