{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "stop_Words = text.ENGLISH_STOP_WORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Mime-Version</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Content-Transfer-Encoding</th>\n",
       "      <th>X-From</th>\n",
       "      <th>...</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Message-Contents</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fri, 15 Jun 2001 11:37:53 -0700 (PDT)</td>\n",
       "      <td>david.allan@enron.com</td>\n",
       "      <td>david.allan@enron.com, e..carter@enron.com, el...</td>\n",
       "      <td>pdated: weekly warrior meeting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>Allan, David &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;22329315.1075853164077.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Team room - EB2942, NC</td>\n",
       "      <td>3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA</td>\n",
       "      <td>-1</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Fri, 15 Jun 2001 11:37:53 -0700 (PDT)</td>\n",
       "      <td>david.allan@enron.com</td>\n",
       "      <td>david.allan@enron.com, e..carter@enron.com, el...</td>\n",
       "      <td>pdated: weekly warrior meeting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>Allan, David &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;22329315.1075853164077.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Team room - EB2942, NC</td>\n",
       "      <td>3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA</td>\n",
       "      <td>-1</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Fri, 15 Jun 2001 11:37:53 -0700 (PDT)</td>\n",
       "      <td>david.allan@enron.com</td>\n",
       "      <td>david.allan@enron.com, e..carter@enron.com, el...</td>\n",
       "      <td>pdated: weekly warrior meeting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>Allan, David &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN=...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;22329315.1075853164077.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Team room - EB2942, NC</td>\n",
       "      <td>3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA</td>\n",
       "      <td>-1</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon, 6 Aug 2001 10:47:06 -0700 (PDT)</td>\n",
       "      <td>stacey.wales@enron.com</td>\n",
       "      <td>stacey.wales@enron.com, e..carter@enron.com, p...</td>\n",
       "      <td>Canceled: Pulp Origination Strategy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>Wales, Stacey &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;19614702.1075853164028.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Dear Team,\\n\\n\\nI would like to brainstorm ide...</td>\n",
       "      <td>3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A</td>\n",
       "      <td>-1</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon, 6 Aug 2001 10:47:06 -0700 (PDT)</td>\n",
       "      <td>stacey.wales@enron.com</td>\n",
       "      <td>stacey.wales@enron.com, e..carter@enron.com, p...</td>\n",
       "      <td>Canceled: Pulp Origination Strategy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>Wales, Stacey &lt;/O=ENRON/OU=NA/CN=RECIPIENTS/CN...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;19614702.1075853164028.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Dear Team,\\n\\n\\nI would like to brainstorm ide...</td>\n",
       "      <td>3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A</td>\n",
       "      <td>-1</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 Unnamed: 0.1                                   Date  \\\n",
       "0          0            0  Fri, 15 Jun 2001 11:37:53 -0700 (PDT)   \n",
       "1          1            1  Fri, 15 Jun 2001 11:37:53 -0700 (PDT)   \n",
       "2          2            2  Fri, 15 Jun 2001 11:37:53 -0700 (PDT)   \n",
       "3          3            3   Mon, 6 Aug 2001 10:47:06 -0700 (PDT)   \n",
       "4          4            4   Mon, 6 Aug 2001 10:47:06 -0700 (PDT)   \n",
       "\n",
       "                     From                                                 To  \\\n",
       "0   david.allan@enron.com  david.allan@enron.com, e..carter@enron.com, el...   \n",
       "1   david.allan@enron.com  david.allan@enron.com, e..carter@enron.com, el...   \n",
       "2   david.allan@enron.com  david.allan@enron.com, e..carter@enron.com, el...   \n",
       "3  stacey.wales@enron.com  stacey.wales@enron.com, e..carter@enron.com, p...   \n",
       "4  stacey.wales@enron.com  stacey.wales@enron.com, e..carter@enron.com, p...   \n",
       "\n",
       "                               Subject Mime-Version  \\\n",
       "0       pdated: weekly warrior meeting          1.0   \n",
       "1       pdated: weekly warrior meeting          1.0   \n",
       "2       pdated: weekly warrior meeting          1.0   \n",
       "3  Canceled: Pulp Origination Strategy          1.0   \n",
       "4  Canceled: Pulp Origination Strategy          1.0   \n",
       "\n",
       "                   Content-Type Content-Transfer-Encoding  \\\n",
       "0  text/plain; charset=us-ascii                      7bit   \n",
       "1  text/plain; charset=us-ascii                      7bit   \n",
       "2  text/plain; charset=us-ascii                      7bit   \n",
       "3  text/plain; charset=us-ascii                      7bit   \n",
       "4  text/plain; charset=us-ascii                      7bit   \n",
       "\n",
       "                                              X-From   ...    X-cc X-bcc  \\\n",
       "0  Allan, David </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...   ...     NaN   NaN   \n",
       "1  Allan, David </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...   ...     NaN   NaN   \n",
       "2  Allan, David </O=ENRON/OU=NA/CN=RECIPIENTS/CN=...   ...     NaN   NaN   \n",
       "3  Wales, Stacey </O=ENRON/OU=NA/CN=RECIPIENTS/CN...   ...     NaN   NaN   \n",
       "4  Wales, Stacey </O=ENRON/OU=NA/CN=RECIPIENTS/CN...   ...     NaN   NaN   \n",
       "\n",
       "                              X-Folder     X-Origin  \\\n",
       "0  \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "1  \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "2  \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "3  \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "4  \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "\n",
       "                      X-FileName  \\\n",
       "0  MCAUSHOL (Non-Privileged).pst   \n",
       "1  MCAUSHOL (Non-Privileged).pst   \n",
       "2  MCAUSHOL (Non-Privileged).pst   \n",
       "3  MCAUSHOL (Non-Privileged).pst   \n",
       "4  MCAUSHOL (Non-Privileged).pst   \n",
       "\n",
       "                                      Message-ID  \\\n",
       "0  <22329315.1075853164077.JavaMail.evans@thyme>   \n",
       "1  <22329315.1075853164077.JavaMail.evans@thyme>   \n",
       "2  <22329315.1075853164077.JavaMail.evans@thyme>   \n",
       "3  <19614702.1075853164028.JavaMail.evans@thyme>   \n",
       "4  <19614702.1075853164028.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    Message-Contents  \\\n",
       "0                             Team room - EB2942, NC   \n",
       "1                             Team room - EB2942, NC   \n",
       "2                             Team room - EB2942, NC   \n",
       "3  Dear Team,\\n\\n\\nI would like to brainstorm ide...   \n",
       "4  Dear Team,\\n\\n\\nI would like to brainstorm ide...   \n",
       "\n",
       "                                           ID Label Scenario  \n",
       "0  3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA    -1      401  \n",
       "1  3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA    -1      402  \n",
       "2  3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA    -1      403  \n",
       "3  3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A    -1      401  \n",
       "4  3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A    -1      402  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/parsed/training.csv\", dtype=str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "#df[\"Message-Contents\"].apply(len)\n",
    "#df[\"Message-Contents\"][16]\n",
    "gross_email = df[df[\"Message-Contents\"].str.contains(\"----\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "'''\n",
    "Email filter class that will hold all of our regexes that we can apply to ALL emails. It is helpful \n",
    "to have it as an objectbecause we can just add the regexes as instance variables and then iterate \n",
    "over all of them using <object>.__dict__.items()\n",
    "'''\n",
    "class Email_filter:\n",
    "     def __init__(self):\n",
    "        ##regexes#\n",
    "        self.quoted = re.compile(r\"^>(\\s)*\", re.MULTILINE)\n",
    "        self.original_or_forward = re.compile(r\"(^(\\s)*-*(\\s)*Original Message(\\s)*-.*$)|(^____.*$)\", re.MULTILINE)\n",
    "        self.metadata = re.compile(r\"(^From:.*$)|(^Sent:.*$)|(^To:.*$)|(^Subject:.*$)|(^Cc:.*$)|(^Date:.*$)|(^Encoding:.*$)\", re.MULTILINE)\n",
    "        self.ole_object = re.compile(r\"(<<.*?>>)+\", re.DOTALL|re.MULTILINE)\n",
    "        self.smtp_header = re.compile(r\"^Message-id:.*?X-Mozilla-Status.*?$\", re.DOTALL|re.MULTILINE)\n",
    "        self.received = re.compile(r\"^Received:(.*?)\\([A-Z]{3}\\)\", re.DOTALL|re.MULTILINE)\n",
    "\n",
    "'''\n",
    "Method to actually filter the emails - creates an email filter object and loops through all of the \n",
    "regexes it contains, running each of them on an email. If the email is a reply, it will first \n",
    "cut out all of the replies.\n",
    "'''\n",
    "def filter_email(s):\n",
    "    if type(s) != str:\n",
    "        return(\"\")\n",
    "    e = Email_filter()\n",
    "    ret = s\n",
    "    for variable_name, regex in e.__dict__.items():  \n",
    "        ret = re.sub(regex, \"\", ret)\n",
    "    \n",
    "    return ret\n",
    "\n",
    "'''\n",
    "Method to cut out all of the replies of emails\n",
    "'''\n",
    "def filter_reply(s):\n",
    "    replies = re.compile(r\"(^(\\s)*-*(\\s)*Original Message.*)\", re.MULTILINE|re.DOTALL)\n",
    "    ret = re.sub(replies, \"\", s)\n",
    "    return ret\n",
    "\n",
    "\n",
    "'''\n",
    "metadata parsing\n",
    "'''\n",
    "from datetime import datetime\n",
    "import re\n",
    "def parse_date(d):\n",
    "    d = d.replace(',', '')\n",
    "    redundancy_filter = d.split(' (')\n",
    "    string = redundancy_filter[0]\n",
    "    datetime_object = datetime.strptime(string, '%a %d %b %Y %X %z')\n",
    "    return datetime_object\n",
    "\n",
    "\n",
    "def x_strip(s):\n",
    "    if s != \"\":\n",
    "        array = re.split(r'</O.*?\\>', s)\n",
    "        array = [x.strip() for x in array]\n",
    "        return array\n",
    "    else:\n",
    "        return([])\n",
    "\n",
    "def to_from_strip(s):\n",
    "    if s != \"\":\n",
    "        return s.split(',')\n",
    "    else:\n",
    "        return([])\n",
    "    \n",
    "def parse_metadata(df):\n",
    "    df['Date'] = df['Date'].apply(parse_date)\n",
    "    df['To'] = df['To'].apply(to_from_strip)\n",
    "    df['From'] = df['From'].apply(to_from_strip)\n",
    "    df['X-To'] = df['X-To'].apply(x_strip)\n",
    "    df['X-From'] = df['X-From'].apply(x_strip)\n",
    "    \n",
    "    return(df)\n",
    "    \n",
    "def full_filter_email(df):\n",
    "    df = df.replace(np.nan, '', regex=True)\n",
    "    df.loc[df[\"Subject\"].str.lower().str.startswith('re')][\"Message-Contents\"].apply(filter_reply)\n",
    "    df[\"Message-Contents\"] = df[\"Message-Contents\"].apply(filter_email)\n",
    "    df = parse_metadata(df)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gross_email_filtered = pd.DataFrame()\n",
    "gross_email_filtered = full_filter_email(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Date</th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Mime-Version</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>Content-Transfer-Encoding</th>\n",
       "      <th>X-From</th>\n",
       "      <th>...</th>\n",
       "      <th>X-cc</th>\n",
       "      <th>X-bcc</th>\n",
       "      <th>X-Folder</th>\n",
       "      <th>X-Origin</th>\n",
       "      <th>X-FileName</th>\n",
       "      <th>Message-ID</th>\n",
       "      <th>Message-Contents</th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Scenario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2001-06-15 11:37:53-07:00</td>\n",
       "      <td>[david.allan@enron.com]</td>\n",
       "      <td>[david.allan@enron.com,  e..carter@enron.com, ...</td>\n",
       "      <td>pdated: weekly warrior meeting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>[Allan, David, ]</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;22329315.1075853164077.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Team room - EB2942, NC</td>\n",
       "      <td>3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA</td>\n",
       "      <td>-1</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2001-06-15 11:37:53-07:00</td>\n",
       "      <td>[david.allan@enron.com]</td>\n",
       "      <td>[david.allan@enron.com,  e..carter@enron.com, ...</td>\n",
       "      <td>pdated: weekly warrior meeting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>[Allan, David, ]</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;22329315.1075853164077.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Team room - EB2942, NC</td>\n",
       "      <td>3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA</td>\n",
       "      <td>-1</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2001-06-15 11:37:53-07:00</td>\n",
       "      <td>[david.allan@enron.com]</td>\n",
       "      <td>[david.allan@enron.com,  e..carter@enron.com, ...</td>\n",
       "      <td>pdated: weekly warrior meeting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>[Allan, David, ]</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;22329315.1075853164077.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Team room - EB2942, NC</td>\n",
       "      <td>3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA</td>\n",
       "      <td>-1</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2001-08-06 10:47:06-07:00</td>\n",
       "      <td>[stacey.wales@enron.com]</td>\n",
       "      <td>[stacey.wales@enron.com,  e..carter@enron.com,...</td>\n",
       "      <td>Canceled: Pulp Origination Strategy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>[Wales, Stacey, ]</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;19614702.1075853164028.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Dear Team,\\n\\n\\nI would like to brainstorm ide...</td>\n",
       "      <td>3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A</td>\n",
       "      <td>-1</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2001-08-06 10:47:06-07:00</td>\n",
       "      <td>[stacey.wales@enron.com]</td>\n",
       "      <td>[stacey.wales@enron.com,  e..carter@enron.com,...</td>\n",
       "      <td>Canceled: Pulp Origination Strategy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>7bit</td>\n",
       "      <td>[Wales, Stacey, ]</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>\\MCAUSHOL (Non-Privileged)\\Calendar</td>\n",
       "      <td>causholli-m</td>\n",
       "      <td>MCAUSHOL (Non-Privileged).pst</td>\n",
       "      <td>&lt;19614702.1075853164028.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Dear Team,\\n\\n\\nI would like to brainstorm ide...</td>\n",
       "      <td>3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A</td>\n",
       "      <td>-1</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 Unnamed: 0.1                       Date  \\\n",
       "0          0            0  2001-06-15 11:37:53-07:00   \n",
       "1          1            1  2001-06-15 11:37:53-07:00   \n",
       "2          2            2  2001-06-15 11:37:53-07:00   \n",
       "3          3            3  2001-08-06 10:47:06-07:00   \n",
       "4          4            4  2001-08-06 10:47:06-07:00   \n",
       "\n",
       "                       From  \\\n",
       "0   [david.allan@enron.com]   \n",
       "1   [david.allan@enron.com]   \n",
       "2   [david.allan@enron.com]   \n",
       "3  [stacey.wales@enron.com]   \n",
       "4  [stacey.wales@enron.com]   \n",
       "\n",
       "                                                  To  \\\n",
       "0  [david.allan@enron.com,  e..carter@enron.com, ...   \n",
       "1  [david.allan@enron.com,  e..carter@enron.com, ...   \n",
       "2  [david.allan@enron.com,  e..carter@enron.com, ...   \n",
       "3  [stacey.wales@enron.com,  e..carter@enron.com,...   \n",
       "4  [stacey.wales@enron.com,  e..carter@enron.com,...   \n",
       "\n",
       "                               Subject Mime-Version  \\\n",
       "0       pdated: weekly warrior meeting          1.0   \n",
       "1       pdated: weekly warrior meeting          1.0   \n",
       "2       pdated: weekly warrior meeting          1.0   \n",
       "3  Canceled: Pulp Origination Strategy          1.0   \n",
       "4  Canceled: Pulp Origination Strategy          1.0   \n",
       "\n",
       "                   Content-Type Content-Transfer-Encoding             X-From  \\\n",
       "0  text/plain; charset=us-ascii                      7bit   [Allan, David, ]   \n",
       "1  text/plain; charset=us-ascii                      7bit   [Allan, David, ]   \n",
       "2  text/plain; charset=us-ascii                      7bit   [Allan, David, ]   \n",
       "3  text/plain; charset=us-ascii                      7bit  [Wales, Stacey, ]   \n",
       "4  text/plain; charset=us-ascii                      7bit  [Wales, Stacey, ]   \n",
       "\n",
       "    ...    X-cc X-bcc                             X-Folder     X-Origin  \\\n",
       "0   ...                \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "1   ...                \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "2   ...                \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "3   ...                \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "4   ...                \\MCAUSHOL (Non-Privileged)\\Calendar  causholli-m   \n",
       "\n",
       "                      X-FileName  \\\n",
       "0  MCAUSHOL (Non-Privileged).pst   \n",
       "1  MCAUSHOL (Non-Privileged).pst   \n",
       "2  MCAUSHOL (Non-Privileged).pst   \n",
       "3  MCAUSHOL (Non-Privileged).pst   \n",
       "4  MCAUSHOL (Non-Privileged).pst   \n",
       "\n",
       "                                      Message-ID  \\\n",
       "0  <22329315.1075853164077.JavaMail.evans@thyme>   \n",
       "1  <22329315.1075853164077.JavaMail.evans@thyme>   \n",
       "2  <22329315.1075853164077.JavaMail.evans@thyme>   \n",
       "3  <19614702.1075853164028.JavaMail.evans@thyme>   \n",
       "4  <19614702.1075853164028.JavaMail.evans@thyme>   \n",
       "\n",
       "                                    Message-Contents  \\\n",
       "0                             Team room - EB2942, NC   \n",
       "1                             Team room - EB2942, NC   \n",
       "2                             Team room - EB2942, NC   \n",
       "3  Dear Team,\\n\\n\\nI would like to brainstorm ide...   \n",
       "4  Dear Team,\\n\\n\\nI would like to brainstorm ide...   \n",
       "\n",
       "                                           ID Label Scenario  \n",
       "0  3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA    -1      401  \n",
       "1  3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA    -1      402  \n",
       "2  3.438093.NRVQMDFEDQVCPPCQH1LAKFSPGODI4KJHA    -1      403  \n",
       "3  3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A    -1      401  \n",
       "4  3.438120.GTSTTLNTZ2LVIWQFALPJFBLRPP2UC0G4A    -1      402  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gross_email_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gross_email_filtered.to_pickle('pickled_data_test.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RE: Transfer of Dana Davis\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "filter_email() got an unexpected keyword argument 'is_reply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6ad5a60bce35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgross_email\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Subject\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50846\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_email\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgross_email\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Message-Contents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50846\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: filter_email() got an unexpected keyword argument 'is_reply'"
     ]
    }
   ],
   "source": [
    "print(gross_email[\"Subject\"][50846])\n",
    "print(filter_email(gross_email[\"Message-Contents\"][50846], is_reply=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check with Maria Lebeau who has been coordinating hiring in the GCP group.\n",
      "\n",
      " -----Original Message-----\n",
      "From: \tMcLoughlin, Hector  \n",
      "Sent:\tFriday, November 02, 2001 2:20 PM\n",
      "To:\tBoals, Adrial; Davis, Dana; Scribner, James; Carrizales, Blanca\n",
      "Cc:\tCampos, Karen E.; Ormston, Kevin; Perkins, Ramona\n",
      "Subject:\tTransfer of Dana Davis\n",
      "\n",
      "I'm seeking some clarification about the possible transfer of Dana Davis into James Scribner's cost center.    It is my understanding that she was to transfer effective 11/1/01?    Was the position by any chance posted?  Do we need to create a position or is it a transfer into an existing position?\n",
      "Thanks for your help, \n",
      "h\n",
      "Please check with Maria Lebeau who has been coordinating hiring in the GCP group.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I'm seeking some clarification about the possible transfer of Dana Davis into James Scribner's cost center.    It is my understanding that she was to transfer effective 11/1/01?    Was the position by any chance posted?  Do we need to create a position or is it a transfer into an existing position?\n",
      "Thanks for your help, \n",
      "h\n"
     ]
    }
   ],
   "source": [
    "print(gross_email[\"Message-Contents\"][50846])\n",
    "print(gross_email_filtered[\"Message-Contents\"][50846])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "emails = pd.read_pickle(\"data/parsed/pickles/pickled_data_test.pickle\")\n",
    "\n",
    "scenario_1 = df[emails[\"Scenario\"] == '401']\n",
    "scenario_2 = df[emails[\"Scenario\"] == '402']\n",
    "scenario_3 = df[emails[\"Scenario\"] == '403']\n",
    "\n",
    "def build_TFIDF_Matrix(df):\n",
    "    vectorizer = TfidfVectorizer(stop_words = stop_Words, min_df = .0005)\n",
    "    vectorized = vectorizer.fit_transform(df[\"Message-Contents\"])\n",
    "\n",
    "    dumb_numbers = [s for s in vectorizer.get_feature_names()\n",
    "    if ((\"0\" in s) or (\"1\" in s) or (\"2\" in s) or (\"3\" in s) or (\"4\" in s)\n",
    "    or (\"5\" in s) or (\"6\" in s) or (\"7\" in s) or (\"8\" in s) or (\"9\" in s) or (\"_\" in s))]\n",
    "\n",
    "    stop_words = text.ENGLISH_STOP_WORDS.union(dumb_numbers)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words = stop_words, min_df = .0005)\n",
    "    vectorized = vectorizer.fit_transform(df[\"Message-Contents\"])\n",
    "    return vectorizer, vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116399, 12135)\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "scenario_1_tfidf_matrix = build_TFIDF_Matrix(scenario_1)\n",
    "feature_names = scenario_1_tfidf_matrix[0].get_feature_names()\n",
    "print(scenario_1_tfidf_matrix[1].toarray().shape)\n",
    "scenario_1_tfidf = scenario_1_tfidf_matrix[1].toarray()\n",
    "np.save('scenario_1_tfidf.npy', scenario_1_tfidf)\n",
    "print(type(feature_names))\n",
    "#scenario_1_data_frame = pd.DataFrame(data = scenario_1_tfidf, index = scenario_1[\"ID\"], columns = scenario_1_tfidf_matrix[0].get_feature_names())\n",
    "# scenario_1_data_frame.to_pickle('data/parsed/pickles/pickled_scenario_1_tfidf.pickle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-baccbe441009>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscenario_1_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scenario_1_tfidf.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario_1_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 419\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scenario_1_loaded = np.load('scenario_1_tfidf.npy')\n",
    "print(scenario_1_loaded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scenario_1_loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-348acd617b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscenario_1_loaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scenario_1_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "print(scenario_1_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116399, 12135)\n"
     ]
    }
   ],
   "source": [
    "scenario_2_tfidf_matrix = build_TFIDF_Matrix(scenario_2)\n",
    "print(scenario_2_tfidf_matrix[1].toarray().shape)\n",
    "\n",
    "scenario_2_tfidf = scenario_2_tfidf_matrix[1].toarray()\n",
    "np.save('scenario_2_tfidf.npy', scenario_2_tfidf)\n",
    "# scenario_2_data_frame = pd.DataFrame(data = scenario_2_tfidf, index = scenario_1[\"ID\"], columns = scenario_2_tfidf_matrix[0].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116399, 12135)\n"
     ]
    }
   ],
   "source": [
    "scenario_3_tfidf_matrix = build_TFIDF_Matrix(scenario_3)\n",
    "print(scenario_3_tfidf_matrix[1].toarray().shape)\n",
    "\n",
    "scenario_3_tfidf = scenario_3_tfidf_matrix[1].toarray()\n",
    "np.save('scenario_3_tfidf.npy', scenario_3_tfidf)\n",
    "# scenario_3_data_frame = pd.DataFrame(data = scenario_3_tfidf, index = scenario_1[\"ID\"], columns = scenario_3_tfidf_matrix[0].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "get_feature_names not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-129538431cb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfull_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memails\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#full_matrix.to_pickle('tfidf_sparse_matrix_full.pickle')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: get_feature_names not found"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-06e5c72fc3b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(vectorizer))\n",
    "print(type(vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing code, small sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 18104)\n"
     ]
    }
   ],
   "source": [
    "emails_short = emails[:20000]\n",
    "vectorized_short = vectorizer.fit_transform(emails_short[\"Message-Contents\"])\n",
    "vectorized_short = pd.DataFrame(vectorized_short.toarray())\n",
    "vectorized_short_T = vectorized_short.T\n",
    "print(vectorized_short.shape)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "# print(len(feature_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names.to_pickle('pickled_names.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "['000033', '000mt', '000s', '003366', '01730', '02081', '025866', '0337', '0354', '04075', '05500', '0551', '0748', '0f91f2936ed2981d3c03c3ee', '100080', '10013', '1004052048', '100mil', '10_year', '110101', '110180', '11112001', '112801_01_01', '112801_01_02', '112801_04_01', '112801_131_01', '112801_131_02', '112801_131_03', '113001', '11579', '116832', '121301', '1299', '13191369', '13214116', '13214487', '1365', '13897', '13905', '13906', '13914', '13917', '13920', '13924', '13938', '13943', '13947', '13950', '13955', '13959', '13966', '13968', '13969', '13972', '13988', '14364', '14367', '14370', '14382', '14383', '14388', '14389', '14390', '14392', '14393', '14399', '14400', '14401', '14404', '14410', '14414', '14420', '14421', '14471', '14472', '14481', '14483', '14485', '14492', '14494', '14495', '14503', '14515', '14519', '14525', '14526', '14542', '14543', '14544', '14566', '14567', '14568', '14570', '14573', '14582', '14586', '14587', '14588', '14595', '14605', '14606', '14612', '14619', '14625', '14632', '14636', '14640101', '14641', '14645', '14653', '14664', '14666', '14677', '14678', '14828', '14829', '14831', '14835', '14836', '14841', '14846', '14849', '14851', '14852', '15087', '15090', '15100', '15101', '15108', '15121', '15122', '15131', '15138', '15143', '15155', '15161', '15162', '15163', '15165', '15264', '15266', '15270', '15276', '15277', '15278', '15286', '15287', '15289', '15290', '15296', '15306', '15313', '15324', '15327', '15376', '15377', '15379', '15389', '15391', '15392', '15393', '15395', '15399', '15401', '15406', '15410', '15412', '15414', '15538', '15547', '15549', '15555', '15556', '15558', '15574', '15576', '15578', '15582', '15584', '15585', '15587', '15589', '1616599', '1655', '16586', '168002', '1700411', '1835', '1845', '1853', '1857', '1864272', '1876388', '1889', '1906', '1912', '1915', '1928', '19991224', '1am', '1cd662bac8bb11d5b373', '1q02', '1z81e74w0345372554', '2001101713', '2001101714', '2001101715', '2001101716', '2001101717', '2001101719', '2001101720', '2001101721', '20136', '2026', '2064', '2070', '20794906', '2089', '2098', '20enronpulp', '20feb', '20oct', '2109', '2121', '2198', '21oct', '2238', '2240', '229mil', '23oct', '24244634', '256m', '2596', '262d4622cb4b23d03bb9eb12', '2701', '2717e', '27888', '2793', '2906', '2943', '300s', '30m', '310001', '31px', '3225845', '3253', '3299', '3360', '3412', '3494f517075bea933bcfaec2', '3537', '3537b', '3652', '365m', '3802', '38px', '391b', '3953c3095e60cab33bf00440', '3q00', '3q01', '401753', '40750', '4102', '41690', '41m', '44207', '4466', '4608', '48hrs', '51652', '5205', '5216', '5234', '53202', '53518ce48b18b4ef3be96926', '5384', '53bil', '5516', '5636', '5836', '6232', '650mil', '65283b5813773ac53bdf7da9', '6583', '660000', '6664', '6665', '6813', '6821', '6872', '6mil', '76102', '77764', '7becdb35c7b2c9523bd78585', '80309', '8250', '837123', '8389', '8497', '85km', '860755', '8758', '8812', '8834', '8849', '8855', '8941', '8c58a4597925991f3be86990', '9010', '9012', '9059651fb91f88043bcdb7df', '9103', '91615', '92118', '94107', '95683347', '9826', '98268758', '999966', '9bacfc3bfdbdf3c13bd590b2', '9e448c79db1f17163bf270b2', '9pt', '_archives', '_eim_fund', 'a0dad2', 'a3a9f06599e2a5f43b8db458', 'aa', 'aantec', 'aaron', 'ab', 'abandoned', 'abb', 'abbreviation', 'abc', 'abee', 'ability', 'abitibi', 'able', 'abn', 'aborted', 'abraded', 'abroad', 'abs', 'absence', 'absent', 'absolute', 'absolutely', 'absorb', 'absorbency', 'absorbents', 'abstraction', 'abundant', 'abusive', 'acacia', 'academic', 'acccording', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'access', 'accessible', 'accessing', 'accident', 'accidental', 'accommodates', 'accompanied', 'accomplished', 'accord', 'accordi', 'according', 'accordingly', 'account', 'accountable', 'accountant', 'accounted', 'accounting', 'accounts', 'accreditation', 'accredited', 'accrued', 'acctno', 'accumulated', 'accuracy', 'accurate', 'accurately', 'accusations', 'accusing', 'accustomed', 'achieve', 'achieves', 'achieving', 'acknowledging', 'acme', 'acocunt', 'acquainted', 'acquire', 'acquired', 'acquirers', 'acquires', 'acquiring', 'acquisition', 'acquisitions', 'acre', 'acres', 'acrobat', 'acronym', 'act', 'action', 'actions', 'active', 'actively', 'activities', 'activity', 'actual', 'actually', 'ad', 'adaptive', 'add', 'added', 'addendum', 'adding', 'addition', 'additional', 'additionally', 'additives', 'addr', 'address', 'address1', 'addressed', 'addresses', 'addressing', 'adhesive', 'adirondack', 'adjacent', 'adjust', 'adjusted', 'adjusts', 'admin', 'administer', 'administration', 'administrative', 'administrator', 'admission', 'admissions', 'admit', 'admitted', 'adobe', 'adopps', 'adrs', 'ads', 'adserver', 'adult', 'advance', 'advanced', 'advances', 'advantage', 'advanz', 'adverse', 'adversely', 'advertise', 'advertised', 'advertisement', 'advertisers', 'advertising', 'advice', 'adviced', 'advise', 'advised', 'advisor', 'advisors', 'advisory', 'aerosolized', 'af', 'afandpa', 'affairs', 'affect', 'affected', 'affecting', 'affects', 'affiliate', 'affiliates', 'affirmed', 'affordable', 'afforded', 'afghanistan', 'afh', 'afraid', 'africa', 'african', 'aftermath', 'afternoon', 'ag', 'age', 'agencies', 'agency', 'agenda', 'agendas', 'agent', 'agents', 'aggregates', 'aggressive', 'aggressively', 'ago', 'agree', 'agreeable', 'agreed', 'agreeing', 'agreement', 'agreements', 'agriculture', 'agro', 'agropak', 'agt', 'agtmail', 'agtmap', 'agtqry1', 'aguilera', 'ahead', 'ahve', 'aid', 'aig', 'ailing', 'aim', 'aimed', 'aiming', 'aims', 'aimsoftny', 'ainsworth', 'air', 'aires', 'ajo', 'akzo', 'ala', 'alabama', 'alan', 'alarms', 'alaska', 'albania', 'albanian', 'albany', 'albeit', 'alberni', 'albert', 'alberta', 'alcantara', 'alert', 'alex', 'alexander', 'alfred', 'align', 'aligned', 'alignment', 'alina', 'alina_np', 'allan', 'allen', 'allergies', 'alliance', 'allied', 'allison', 'allocation', 'allotment', 'allow', 'allowed', 'allowing', 'allows', 'allyn', 'alt', 'alta', 'alternate', 'alternating', 'alternative', 'alternatives', 'altmark', 'altogether', 'alumni', 'amar', 'amazon', 'ambassador', 'ambitious', 'amendments', 'amends', 'america', 'american', 'americans', 'ami', 'amidst', 'amounted', 'amounts', 'amp', 'ample', 'amro', 'amsterdam', 'amy', 'ana', 'analyses', 'analysis', 'analyst', 'analysts', 'analytical', 'analytics', 'analyze', 'analyzed', 'anatomy', 'anchorage', 'ancient', 'ancillary', 'andrea', 'andrew', 'andy', 'anecdotal', 'angeles', 'animesh', 'anlagen', 'ann', 'anna', 'anne', 'annex_28', 'annexes', 'anniversary', 'annou', 'announce', 'announced', 'announcement', 'announcements', 'announces', 'annual', 'annualized', 'annually', 'annuities', 'annum', 'anomaly', 'anonredir', 'answer', 'answered', 'answering', 'answers', 'anthrax', 'antibiotics', 'anticipate', 'anticipated', 'anticipation', 'antidumping', 'antitrust', 'anybody', 'anymore', 'anytime', 'anyways', 'aol', 'aoustin', 'apa', 'apart', 'apartment', 'apki', 'apologise', 'apologize', 'app', 'apparel', 'apparently', 'appear', 'appeared', 'appears', 'append', 'appended', 'appleton', 'applicants', 'application', 'applications', 'applied', 'apply', 'applying', 'appointed', 'appointing', 'appointment', 'appplication', 'appraisals', 'appreciate', 'appreciated', 'approach', 'approached', 'approaching', 'appropriate', 'approval', 'approved', 'approves', 'approving', 'approximately', 'april', 'aprs', 'aracruz', 'arauco', 'arbitrage', 'architecturally', 'archive', 'archiving', 'ardmore', 'area', 'areas', 'aren', 'arg2_name', 'arg2_tie', 'arg2_value', 'arg3_name', 'arg3_tie', 'arg3_value', 'argentina', 'arguably', 'argue', 'arial', 'arising', 'arizona', 'arjo', 'ark', 'arles', 'arlington', 'armp', 'arnhem', 'arnie', 'arrange', 'arranged', 'arrangement', 'arrangements', 'arranger', 'arrays', 'arrears', 'arrij', 'arritur', 'arrival', 'arrive', 'arrived', 'arrogant', 'article', 'articles', 'artist', 'arts', 'asap', 'asf', 'asherf', 'asia', 'asian', 'asiapulse', 'aside', 'ask', 'aske', 'asked', 'asking', 'asks', 'asp', 'aspapel', 'aspect', 'aspects', 'asps', 'asserted', 'assess', 'assessment', 'asset', 'assets', 'assign', 'assigned', 'assignments', 'assistance', 'assistant', 'assistantship', 'assistantships', 'assists', 'assn', 'assoc', 'associate', 'associated', 'associates', 'association', 'assume', 'assumed', 'assuming', 'assumption', 'assurance', 'astral', 'atelier', 'athens', 'athletes', 'atlanta', 'attach', 'attached', 'attaching', 'attachment', 'attachments', 'attack', 'attacks', 'attempt', 'attempted', 'attempts', 'attend', 'attendance', 'attending', 'attitude', 'attivita', 'attorneys', 'attract', 'attractive', 'attractiveness', 'attributed', 'attributes', 'auction', 'audience', 'audio', 'aug', 'august', 'augusta', 'auml', 'austin', 'australia', 'australian', 'austria', 'authorities', 'authority', 'authorized', 'authors', 'auto', 'autogenerated', 'automated', 'automatic', 'automatically', 'automation', 'automobile', 'autonomously', 'autos', 'autumn', 'availability', 'available', 'avaya', 'ave', 'avenue', 'aver', 'average', 'averaging', 'avoid', 'avoids', 'avr', 'avril', 'awaited', 'awaiting', 'awaits', 'award', 'awarded', 'aware', 'away', 'awesome', 'awhile', 'axed', 'ayesha', 'aylesford', 'azurix', 'b545e2c8c99a11d5964130c69512c66e', 'ba', 'bachelor', 'bachelors', 'backbone', 'backed', 'background', 'backout', 'backup', 'backups', 'bad', 'badge', 'badges', 'badly', 'baertsoen', 'bag', 'bags', 'bailed', 'balance', 'balanced', 'balances', 'bales', 'ball', 'ballapur', 'ballarpur', 'ballroom', 'bamboo', 'banc', 'bancorp', 'band', 'bangerter', 'bank', 'banking', 'bankrupt', 'bankruptcy', 'bar', 'barada', 'barely', 'bargain', 'barhouston', 'barney', 'barometer', 'baron', 'barr', 'barren', 'barry', 'bart', 'base', 'based', 'basic', 'basics', 'basing', 'basis', 'bath', 'batista', 'battered', 'battle', 'bay', 'bayer', 'bayerische', 'baylor', 'bb', 'bc', 'bct', 'bcti', 'bctmp', 'bea93575440c1e5e3bccf940', 'beacons', 'bear', 'bears', 'beat', 'bed', 'bedford', 'bee', 'beer', 'began', 'begged', 'begin', 'beginning', 'begins', 'begun', 'behalf', 'behaviors', 'beijing', 'bej', 'beje', 'bek', 'belgium', 'belief', 'believe', 'believed', 'believes', 'belkorp', 'bellingham', 'belo', 'belt', 'belts', 'ben', 'benbow', 'benchmark', 'benchmarks', 'benefit', 'benefited', 'benefiting', 'benefits', 'bere', 'berger', 'berlin', 'berman', 'bermuda', 'besicorp', 'best', 'beteiligungs', 'better', 'betting', 'bexel', 'bf1ffd98e9d6f1d93bd07a4f', 'bgcolor', 'bhadrachalam', 'bhatia', 'bhk', 'bhkp', 'bi', 'bias', 'bid', 'bidding', 'bids', 'big', 'bigger', 'biggest', 'billing', 'billion', 'billions', 'bills', 'billy', 'bilt', 'bin', 'binders', 'binding', 'bintulu', 'biofuels', 'biological', 'bir', 'birch', 'birthday', 'bisk', 'bit']\n"
     ]
    }
   ],
   "source": [
    "print(type(vectorized_short))\n",
    "print(vectorizer.get_feature_names()[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA\n",
    "We take the tf-idf matrix and run TruncatedSVD on it with whatever number of dimensions we want to reduce it to. The issue is I don't believe I can pull the categories. I can pull the highest ranking words from each category, but there is no storage of the category iteself. Currently a WIP on pulling each category, but for right now I'm doing the dot product so I can map term to term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stop_Words, min_df = .0005)\n",
    "lsa = np.load('lsa_output.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Implemented without stemming to better work with current tf-idf code and the nasty documents\n",
    "\n",
    "svd = TruncatedSVD(n_components = 100, algorithm='randomized')\n",
    "\n",
    "# lsa = svd.fit_transform(scenario_1_tfidf)\n",
    "\n",
    "def get_closest(term, vectorizer ,model):\n",
    " \n",
    "    index = vectorizer.vocabulary_[term]\n",
    "\n",
    "    model = np.dot(model, model.T)\n",
    "    search_space = np.concatenate( (model[index][:index] , model[index][(index+1):]) )  \n",
    "    out = np.argmax(search_space)\n",
    "    if out<index:\n",
    "        return vectorizer.get_feature_names()[out]\n",
    "    else:\n",
    "        return vectorizer.get_feature_names()[(out+1)]\n",
    "    \n",
    "def get_closest_document(index, vectorizer, model):\n",
    "    model = np.dot(model, model.T)\n",
    "    search_space = np.concatenate( (model[index][:index] , model[index][(index+1):]) )  \n",
    "    out = np.argmax(search_space)\n",
    "    if out<index:\n",
    "        return emails_short[\"Message-Contents\"][out]\n",
    "    else:\n",
    "        return emails_short[\"Message-Contents\"][out+1]\n",
    "#     if out<index:\n",
    "#         return vectorizer.get_feature_names()[out]\n",
    "#     else:\n",
    "#         return vectorizer.get_feature_names()[(out+1)]\n",
    "    \n",
    "def get_k_closest(k,term,vectorizer,model):\n",
    "    index = vectorizer.vocabulary_[term]\n",
    "    print(\"index: \", index)\n",
    "\n",
    "    # This is one approach where we take the dot product\n",
    "    # This results in a mapping of terms to terms based on similarity to each other\n",
    "    model = np.dot(model,model.T)\n",
    "    print(model.shape)\n",
    "\n",
    "    closest_terms = {}\n",
    "    print(len(model))\n",
    "    for i in range(len(model)):\n",
    "        closest_terms[vectorizer.get_feature_names()[i]] = model[index][i]\n",
    "    #print(closest_terms)\n",
    "    sorted_list = sorted(closest_terms , key = lambda l : closest_terms[l])\n",
    "    return sorted_list[::-1][0:k]\n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 44374 is out of bounds for axis 0 with size 20000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8738df7332e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_closest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_k_closest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-46f35ea65744>\u001b[0m in \u001b[0;36mget_closest\u001b[0;34m(term, vectorizer, model)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msearch_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 44374 is out of bounds for axis 0 with size 20000"
     ]
    }
   ],
   "source": [
    "print(get_closest(\"stock\", vectorizer, lsa))\n",
    "print(get_k_closest(5, \"stock\", vectorizer, lsa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_closest_document example NOT MAINTAINED\n",
    "\n",
    "Outputs the email that is the most similar to the one given (by index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(get_closest_document(510, vectorizer, lsa))\n",
    "print(\"--------------------------------------------------------------\")\n",
    "print(emails_short[\"Message-Contents\"][510])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling the categories\n",
    "\n",
    "Here we have the words that form the category for category \\#50. \"Monika\" is the defining person of category \\#50, as are \"wrap\", \"market\", and \"report\".\n",
    "\n",
    "You can take absolute values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the LSA output\n",
    "svd_components = np.load('svd-components.npy')\n",
    "\n",
    "'''\n",
    "Return terms and values of the whole category with values rounded to the \n",
    "7th decimal place.\n",
    "'''\n",
    "def full_category(svd_components, feature_names, index):\n",
    "    temp = list(reversed(svd_components[index].argsort()))\n",
    "    svd_sorted = svd_components[index][temp]   \n",
    "    print('\\n'.join([\"%0.7f*%s\" % (coef, feat) for coef, feat in zip(svd_sorted, np.array(feature_names)[temp])]))\n",
    "\n",
    "'''\n",
    "Take the top 20 terms for each topic and save in a pandas dataframe. Store\n",
    "the values in a separate dataframe\n",
    "'''\n",
    "def top_terms(svd_components, feature_names):\n",
    "    term_columns = []\n",
    "    value_columns = []\n",
    "    for i in range(0, 20):\n",
    "        term_columns.append(\"term\" + str(i))\n",
    "        value_columns.append(\"value\" + str(i))\n",
    "    terms_df = pd.DataFrame(columns=term_columns)\n",
    "    values_df = pd.DataFrame(columns=value_columns)\n",
    "    for i in range(0, 100):\n",
    "        temp = list(reversed(svd_components[i].argsort()))\n",
    "        svd_sorted = svd_components[i][temp]\n",
    "        terms_row = []\n",
    "        values_row = []\n",
    "        for j in range(0, 20):\n",
    "            terms_row.append(np.array(feature_names)[temp][j])\n",
    "            values_row.append(svd_sorted[j])\n",
    "        terms_df.loc[i] = terms_row\n",
    "        values_df.loc[i] = values_row\n",
    "    return terms_df, values_df         \n",
    "\n",
    "\n",
    "terms_df, values_df = top_terms(svd_components, feature_names)\n",
    "terms_df.to_pickle('LSA_dataframes/pickled_LSA_terms.pickle')\n",
    "values_df.to_pickle('LSA_dataframes/pickled_LSA_values.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('feature_names_short.pickle', 'wb') as f:\n",
    "    pickle.dump(feature_names_short, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [term0, term1, term2, term3, term4, term5, term6, term7, term8, term9, term10, term11, term12, term13, term14, term15, term16, term17, term18, term19]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
